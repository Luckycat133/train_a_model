# 灵猫墨韵 - 技术架构文档

## 系统架构概览

灵猫墨韵是一个专为古典中文文本优化的轻量级语言模型训练与推理系统。本文档详细说明项目的技术架构、各模块之间的关系以及数据流转。

### 系统架构图

```
+---------------------------+
|       用户交互层          |
+---------------------------+
          |      ^
          v      |
+---------------------------+
|       核心模块层          |
|---------------------------|
| +----------+ +----------+ |
| | 训练模块 | | 生成模块 | |
| +----------+ +----------+ |
|        |          |       |
| +----------+ +----------+ |
| | 分词模块 | | 处理模块 | |
| +----------+ +----------+ |
+---------------------------+
          |      ^
          v      |
+---------------------------+
|       数据管理层          |
+---------------------------+
```

## 核心模块说明

### 1. 训练模块 (train_model.py)

负责模型训练的核心模块，实现了基于Transformer架构的语言模型训练。

**主要组件**：
- `SimpleTransformer` - 基于Transformer的模型架构
- `PositionalEncoding` - 位置编码实现
- `LMDataset` - 语言模型数据集处理
- `train_model` - 训练入口函数

**技术特点**：
- 支持混合精度训练 (FP16)
- 梯度累积实现
- 自动断点恢复
- 夜间低功耗模式
- 自适应学习率调度

### 2. 生成模块 (generate.py)

处理模型推理和文本生成的模块。

**主要组件**：
- 文本生成算法
- 采样策略实现 (Top-K, Top-p)
- 温度控制机制

**技术特点**：
- 支持不同生成策略
- 可控制随机性参数
- 动态输出长度管理

### 3. 分词模块 (tokenizer.py)

负责文本分词、编码和解码的模块。

**主要组件**：
- `ClassicalTokenizer` - 为古典中文优化的分词器
- BPE分词算法实现
- 最大匹配分词实现

**技术特点**：
- 多种分词策略支持
- 高效词表管理
- 针对古典中文的特殊优化

### 4. 处理模块 (processor.py)

数据预处理和后处理的模块。

**主要组件**：
- `ProcessorMain` - 主处理器类
- `DataProcessor`, `TextCleaner` 等子处理器
- 数据转换与格式化工具

**技术特点**：
- 模块化处理流程
- 多进程并行处理
- 内存高效处理大规模数据

## 数据流转

### 训练流程数据流

1. **原始文本** → **处理模块**
   - 清洗和标准化文本
   - 格式转换 (JSON/JSONL)
   - 质量过滤

2. **处理模块** → **分词模块**
   - 文本分割成训练样本
   - 词表构建和学习

3. **分词模块** → **训练模块**
   - Token编码
   - 批量数据准备
   - 上下文窗口构建

4. **训练模块** → **模型文件**
   - 模型权重保存
   - 检查点创建
   - 训练统计信息

### 生成流程数据流

1. **用户输入** → **分词模块**
   - 提示文本分词
   - Token编码

2. **分词模块** → **生成模块**
   - 编码序列输入模型
   - 应用生成策略

3. **生成模块** → **用户输出**
   - Token解码为文本
   - 格式化输出

## 系统扩展性设计

### 模块化架构

每个模块都设计为独立组件，具有明确定义的接口，便于替换或升级：

- 可替换不同的Transformer实现
- 支持多种分词策略
- 灵活的数据处理管道

### 配置驱动设计

系统基于配置驱动，关键参数通过配置文件或命令行参数提供：

- `config/config.yaml` - 核心配置文件
- 命令行参数优先级高于配置文件

### 兼容性考虑

- 支持多种硬件平台 (NVIDIA GPU, Apple Silicon, CPU)
- 优化不同算力设备的性能
- 自适应资源使用策略

## 核心算法

### 1. 模型架构

项目使用简化版Transformer架构，关键点包括：

- 多头自注意力机制
- 位置编码实现
- 残差连接与层归一化
- 前馈神经网络

### 2. 分词策略

针对古典中文的特点，采用了多种分词方法结合：

- 基于BPE的子词分词
- 字符级分词作为后备
- 最大匹配算法优化处理

### 3. 训练优化

- 梯度累积 - 支持在有限显存下使用更大批次
- 混合精度训练 - 加速计算并减少内存使用
- 学习率调度 - 自适应调整训练过程

## 性能考量

### 1. 内存优化

- 批处理优化
- 梯度检查点技术
- 自适应批次大小

### 2. 计算效率

- 针对不同设备的特定优化
- 自动资源限制
- 夜间模式降低资源占用

### 3. 存储优化

- 清理工具管理临时文件
- 优化检查点保存策略
- 智能日志管理

## 安全与稳定性

- 训练中断保护
- 数据一致性保障
- 异常处理与恢复机制
- 定期检查点

## 技术堆栈

- **核心框架**：PyTorch
- **数据处理**：NumPy, Pandas
- **可视化**：Matplotlib
- **工具库**：tqdm, yaml, json

---

*文档最后更新: 2025年3月18日* 