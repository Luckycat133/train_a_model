#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
çµçŒ«å¢¨éŸµå¤å…¸æ–‡å­¦æ•°æ®å¤„ç†ç³»ç»Ÿ - ä¸»å¯åŠ¨å™¨
ç‰ˆæœ¬: v2.1.2
"""

import os
import sys
import argparse
import logging
import random
import shutil
import glob
import time
import queue
import threading
import gc
from datetime import datetime
from pathlib import Path
import json
import yaml
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from tqdm import tqdm
from typing import Dict, List, Any, Optional, Set, Tuple
import itertools
from contextlib import contextmanager
import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent
sys.path.append(str(ROOT_DIR))

from processors.data_processor import DataProcessor
from processors.text_cleaner import TextCleaner
from processors.structure_organizer import StructureOrganizer
from processors.data_synthesizer import DataSynthesizer
from processors.dictionary_generator import DictionaryGenerator


class ProcessorMain:
    """çµçŒ«å¢¨éŸµå¤å…¸æ–‡å­¦æ•°æ®å¤„ç†ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config_path="config/config.yaml"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨ä¸»ç±»
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        self.logger.info("åˆå§‹åŒ–çµçŒ«å¢¨éŸµå¤å…¸æ–‡å­¦æ•°æ®å¤„ç†ç³»ç»Ÿ")
        
        # ä¿å­˜é…ç½®è·¯å¾„
        self.config_path = config_path
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_cleaner = TextCleaner()
        self.structure_organizer = StructureOrganizer(self.config)
        self.data_processor = DataProcessor(self.config, self.text_cleaner, self.structure_organizer)
        
        # ä¸»è¦è·¯å¾„
        self.main_dir = Path(".")
        self.dataset_dir = self.main_dir / "dataset"
        self.logs_dir = self.main_dir / "logs"
        self.temp_dirs = [self.main_dir / "temp_data", self.main_dir / "cleanup_temp"]
        self.model_weights_dir = self.main_dir / "model_weights"
        
        # ç¡®ä¿å…³é”®ç›®å½•å­˜åœ¨
        self.dataset_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        self.model_weights_dir.mkdir(exist_ok=True)
    
    def _setup_logger(self):
        """
        è®¾ç½®æ—¥å¿—
        
        Returns:
            æ—¥å¿—å™¨å®ä¾‹
        """
        logger = logging.getLogger("LingmaoMoyun")
        logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.main_dir = Path(__file__).resolve().parent
        self.logs_dir = self.main_dir / "logs"
        self.logs_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¸ºå„æ¨¡å—åˆ›å»ºå­æ—¥å¿—ç›®å½•
        self.processor_logs_dir = self.logs_dir / "processor"
        self.train_model_logs_dir = self.logs_dir / "train_model"
        self.generate_logs_dir = self.logs_dir / "generate"
        self.data_logs_dir = self.logs_dir / "data"
        
        # ç¡®ä¿æ‰€æœ‰æ—¥å¿—ç›®å½•å­˜åœ¨
        self.processor_logs_dir.mkdir(exist_ok=True, parents=True)
        self.train_model_logs_dir.mkdir(exist_ok=True, parents=True)
        self.generate_logs_dir.mkdir(exist_ok=True, parents=True)
        self.data_logs_dir.mkdir(exist_ok=True, parents=True)
        
        # ä½¿ç”¨å½“å‰æ—¥æœŸæ—¶é—´åˆ›å»ºå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶å
        log_dir = self.processor_logs_dir  # å¤„ç†å™¨çš„æ—¥å¿—å­˜æ”¾åœ¨processorå­ç›®å½•
        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = log_dir / f"processor_{current_time}.log"
        
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨ - æ–‡ä»¶é‡Œä¿ç•™è¯¦ç»†æ—¥å¿—æ ¼å¼
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ - ç®€åŒ–æ§åˆ¶å°è¾“å‡ºæ ¼å¼ï¼Œä»…æ˜¾ç¤ºæ¶ˆæ¯
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(message)s')
        console_handler.setFormatter(console_formatter)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤æ·»åŠ 
        if logger.handlers:
            logger.handlers.clear()
            
        # æ·»åŠ å¤„ç†å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _load_config(self, config_path):
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            é…ç½®å­—å…¸
        """
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            self.logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def process_data(self, force=False):
        """å¤„ç†æ•°æ®"""
        self.logger.info("å¼€å§‹å¤„ç†æ•°æ®...")
        
        # åˆå§‹åŒ–å†…å­˜æ± å’Œä»»åŠ¡é˜Ÿåˆ—
        memory_pool = MemoryPool(max_size=self.config.get("memory_limit", 1024 * 1024 * 1024))
        task_queue = queue.Queue()
        
        # é…ç½®å¹¶è¡Œå¤„ç†
        num_workers = min(os.cpu_count(), self.config.get("max_workers", 4))
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            # æ‰¹é‡é¢„åŠ è½½æ•°æ®
            batch_size = self.config.get("batch_size", 1000)
            for batch in self._batch_data_loader(batch_size):
                task_queue.put(batch)
            
            # å¹¶è¡Œå¤„ç†æ•°æ®
            futures = []
            while not task_queue.empty():
                batch = task_queue.get()
                future = executor.submit(self._process_batch, batch, memory_pool)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in tqdm(futures, desc="å¤„ç†æ•°æ®æ‰¹æ¬¡"):
                try:
                    result = future.result()
                    if not result:
                        self.logger.warning("éƒ¨åˆ†æ•°æ®å¤„ç†å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"æ•°æ®å¤„ç†å‡ºé”™: {e}")
                    return False
                finally:
                    memory_pool.cleanup()
        
        return True
    
    def _batch_data_loader(self, batch_size):
        """æ”¹è¿›çš„æ‰¹é‡æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜å ç”¨"""
        # ä½¿ç”¨Path.globè€Œä¸æ˜¯list(glob.glob())é¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡ä»¶å
        data_path = Path(self.dataset_dir)
        # ä½¿ç”¨è¿­ä»£å™¨å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºå¤§åˆ—è¡¨
        file_iter = data_path.glob("**/*.txt")
        
        current_batch = []
        for file_path in file_iter:
            current_batch.append(file_path)
            if len(current_batch) >= batch_size:
                yield current_batch
                current_batch = []
            
        # è¿”å›æœ€åä¸è¶³batch_sizeçš„æ‰¹æ¬¡
        if current_batch:
            yield current_batch
    
    def _process_batch(self, batch, memory_pool):
        """å¤„ç†å•ä¸ªæ•°æ®æ‰¹æ¬¡"""
        try:
            with memory_pool.acquire() as mem:
                data_synthesizer = DataSynthesizer(self.config, text_cleaner=self.text_cleaner)
                return data_synthesizer.process_batch(batch, mem)
        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            return False

class MemoryPool:
    """å†…å­˜æ± ç®¡ç†å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, max_size, cleanup_threshold=0.8):
        self.max_size = max_size
        self.current_size = 0
        self.lock = threading.Lock()
        self.cleanup_threshold = cleanup_threshold
        self.last_cleanup_time = time.time()
        self.cleanup_interval = 60  # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # ç›‘æ§ç»Ÿè®¡
        self.cleanup_count = 0
        self.peak_memory = 0
    
    @contextmanager
    def acquire(self):
        """è·å–å†…å­˜å—ï¼ŒåŠ å…¥è‡ªåŠ¨æ¸…ç†æœºåˆ¶"""
        current_time = time.time()
        try:
            with self.lock:
                # å®šæœŸæ£€æŸ¥æˆ–å†…å­˜ä½¿ç”¨è¶…é˜ˆå€¼æ—¶æ¸…ç†
                if (self.current_size >= self.max_size * self.cleanup_threshold or 
                    current_time - self.last_cleanup_time > self.cleanup_interval):
                    self.cleanup()
                    self.last_cleanup_time = current_time
                self.current_size += 1
                # è®°å½•å³°å€¼å†…å­˜ä½¿ç”¨
                self.peak_memory = max(self.peak_memory, self.current_size)
            yield self
        finally:
            with self.lock:
                self.current_size -= 1
    
    def cleanup(self):
        """å¢å¼ºçš„å†…å­˜æ¸…ç†"""
        # å¼ºåˆ¶åƒåœ¾æ”¶é›†
        gc.collect()
        self.cleanup_count += 1
        
        # è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        current_memory_usage = memory_info.rss / (1024 * 1024)  # MB
        
        # è®°å½•æ¸…ç†ä¿¡æ¯
        logging.debug(f"å†…å­˜æ¸…ç†æ‰§è¡Œ #{self.cleanup_count}: å½“å‰å†…å­˜ä½¿ç”¨ {current_memory_usage:.2f}MB")

    def get_stats(self):
        """è·å–å†…å­˜æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "current_size": self.current_size,
            "peak_memory": self.peak_memory,
            "cleanup_count": self.cleanup_count,
            "max_size": self.max_size
        }

    def _get_data_logger(self, operation_name):
        """
        è·å–æ•°æ®å¤„ç†ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
        
        Args:
            operation_name: æ“ä½œåç§°ï¼Œå¦‚'fix'ã€'optimize'ç­‰
            
        Returns:
            æ•°æ®å¤„ç†ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
        """
        try:
            # è®¾ç½®dataä¸“ç”¨æ—¥å¿—
            data_logger = logging.getLogger(f"LingmaoMoyun.DataProcessor.{operation_name}")
            
            # åˆ›å»ºdataæ—¥å¿—æ–‡ä»¶
            current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = self.data_logs_dir / f"data_{operation_name}_{current_time}.log"
            
            # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            # è®¾ç½®æ ¼å¼
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            
            # æ·»åŠ å¤„ç†å™¨
            data_logger.addHandler(file_handler)
            
            # è®°å½•å¼€å§‹ä¿¡æ¯
            data_logger.info(f"å¼€å§‹æ•°æ®{operation_name}æ“ä½œ")
            self.logger.info(f"æ•°æ®{operation_name}æ—¥å¿—å°†è®°å½•åˆ°: {log_file}")
            
            return data_logger
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®å¤„ç†æ—¥å¿—æ—¶å‡ºé”™: {e}")
            return self.logger  # å¦‚æœå‡ºé”™ï¼Œè¿”å›ä¸»æ—¥å¿—è®°å½•å™¨


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çµçŒ«å¢¨éŸµå¤å…¸æ–‡å­¦æ•°æ®å¤„ç†ç³»ç»Ÿ")
    parser.add_argument("--config", type=str, default="config/config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†æ—§æ–‡ä»¶å’Œç›®å½•")
    parser.add_argument("--optimize", action="store_true", help="ä¼˜åŒ–æ•°æ®ï¼ˆåˆå¹¶ã€æ‹†åˆ†æ•°æ®é›†ï¼‰")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯æ•°æ®å®Œæ•´æ€§")
    parser.add_argument("--process", action="store_true", help="å¤„ç†æ•°æ®")
    parser.add_argument("--all", action="store_true", help="æ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼ˆå¤„ç†ã€ä¼˜åŒ–ã€éªŒè¯ï¼‰")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = ProcessorMain(config_path=args.config)
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†ä»»ä½•æ“ä½œï¼Œå¦‚æœæ²¡æœ‰ï¼Œæ‰§è¡Œå®Œæ•´æµç¨‹
    if not any([args.clean, args.optimize, args.validate, args.process, args.all]):
        print("æ²¡æœ‰æŒ‡å®šæ“ä½œï¼Œæ‰§è¡Œé»˜è®¤å®Œæ•´å¤„ç†æµç¨‹...")
        try:
            processor.logger.info("æ‰§è¡Œé»˜è®¤å®Œæ•´å¤„ç†æµç¨‹...")
            
            # æ­¥éª¤1: å¤„ç†æ•°æ®
            processor.logger.info("ã€æ­¥éª¤1/3ã€‘å¤„ç†æ•°æ®...")
            process_success = processor.process_data(force=args.force)
            
            # æ­¥éª¤2: ä¼˜åŒ–æ•°æ®
            processor.logger.info("ã€æ­¥éª¤2/3ã€‘ä¼˜åŒ–æ•°æ®...")
            if process_success:
                optimize_success = processor.optimize_data()
            else:
                processor.logger.warning("å‰åºæ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡æ•°æ®ä¼˜åŒ–")
                optimize_success = False
            
            # æ­¥éª¤3: éªŒè¯æ•°æ®
            processor.logger.info("ã€æ­¥éª¤3/3ã€‘éªŒè¯æ•°æ®...")
            if process_success:
                validate_success = processor.validate_data_integrity()
            else:
                processor.logger.warning("å‰åºæ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡æ•°æ®éªŒè¯")
                validate_success = False
            
            # è¾“å‡ºæ€»ç»“ - ä½¿ç”¨ç®€æ´çš„å›¾å½¢æ ¼å¼è¾“å‡ºç»“æœ
            from termcolor import colored
            print("\n" + "="*50)
            print(colored("ğŸ’¼ å¤„ç†æµç¨‹å®Œæˆæ‘˜è¦", "cyan", attrs=["bold"]))
            print("="*50)
            print(f"æ•°æ®å¤„ç†: {colored('âœ“ æˆåŠŸ', 'green') if process_success else colored('âœ— å¤±è´¥', 'red')}")
            print(f"æ•°æ®ä¼˜åŒ–: {colored('âœ“ æˆåŠŸ', 'green') if optimize_success else colored('âœ— å¤±è´¥', 'red')}")
            print(f"æ•°æ®éªŒè¯: {colored('âœ“ æˆåŠŸ', 'green') if validate_success else colored('âœ— å¤±è´¥', 'red')}")
            print("="*50)
            
            return all([process_success, optimize_success, validate_success])
        except KeyboardInterrupt:
            processor.logger.warning("ç”¨æˆ·ä¸­æ–­äº†å¤„ç†æµç¨‹")
            return False
    
    # æ‰§è¡Œç”¨æˆ·æŒ‡å®šçš„æ“ä½œ
    success = True
    
    try:
        if args.all:
            processor.logger.info("æ‰§è¡Œæ‰€æœ‰æ­¥éª¤")
            if args.clean:
                processor.cleanup_old_files()
            process_success = processor.process_data(force=args.force)
            if process_success:
                optimize_success = processor.optimize_data()
                validate_success = processor.validate_data_integrity()
                success = all([process_success, optimize_success, validate_success])
            else:
                success = False
        else:
            if args.clean:
                processor.logger.info("æ¸…ç†æ—§æ–‡ä»¶")
                processor.cleanup_old_files()
            
            if args.process:
                processor.logger.info("å¼€å§‹å¤„ç†æ•°æ®...")
                if not processor.process_data(force=args.force):
                    processor.logger.error("æ•°æ®å¤„ç†å¤±è´¥")
                    success = False
            
            if args.optimize:
                processor.logger.info("å¼€å§‹ä¼˜åŒ–æ•°æ®...")
                if not processor.optimize_data():
                    processor.logger.error("æ•°æ®ä¼˜åŒ–å¤±è´¥")
                    success = False
            
            if args.validate:
                processor.logger.info("å¼€å§‹éªŒè¯æ•°æ®å®Œæ•´æ€§...")
                if not processor.validate_data_integrity():
                    processor.logger.error("æ•°æ®éªŒè¯å¤±è´¥")
                    success = False
    except KeyboardInterrupt:
        processor.logger.warning("å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
        return False
    
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
