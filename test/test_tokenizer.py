#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†è¯å™¨(tokenizer.py)æµ‹è¯•è„šæœ¬
ç”¨äºå…¨é¢æµ‹è¯•åˆ†è¯å™¨çš„åŠŸèƒ½ã€æ€§èƒ½å’Œè¾¹ç•Œæ¡ä»¶
"""

import os
import sys
import time
import json
import logging
import tempfile
import shutil
import random
import pytest
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("åˆ†è¯å™¨æµ‹è¯•")

# å¯¼å…¥åˆ†è¯å™¨ç»„ä»¶
sys.path.append(str(Path(__file__).resolve().parent.parent))
from tokenizer import ClassicalTokenizer

# åŸºç¡€åˆ†è¯æµ‹è¯•
@pytest.mark.parametrize("text_input, expected_tokens", [
    ("ä½ å¥½ä¸–ç•Œ", ["ä½ å¥½", "ä¸–ç•Œ"]),
    ("åºŠå‰æ˜æœˆå…‰", ["åºŠå‰", "æ˜æœˆå…‰"]),
    ("Hello world!", ["Hello", "world", "!"]),
])
def test_basic_tokenization(text_input, expected_tokens):
    """æµ‹è¯•åŸºç¡€åˆ†è¯åŠŸèƒ½"""
    tokenizer = ClassicalTokenizer()
    assert tokenizer.tokenize(text_input) == expected_tokens

# ä¸åŒmethodæµ‹è¯•
@pytest.mark.parametrize("method, text_input, expected_tokens", [
    ("max_match", "æ˜¥é£åˆç»¿æ±Ÿå—å²¸", ["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"]),
    ("bpe", "Hello world!", ["Hello", "world", "!"]),
    ("auto", "è½éœä¸å­¤é¹œé½é£", ["è½éœ", "ä¸", "å­¤é¹œ", "é½é£"]),
])
def test_different_methods(method, text_input, expected_tokens):
    """æµ‹è¯•ä¸åŒåˆ†è¯æ–¹æ³•"""
    tokenizer = ClassicalTokenizer()
    assert tokenizer.tokenize(text_input, method=method) == expected_tokens

# ä¸åŒtext_typeæµ‹è¯•
@pytest.mark.parametrize("text_type, text_input, expected_tokens", [
    ("poem", "å…³å…³é›é¸ ï¼Œåœ¨æ²³ä¹‹æ´²ã€‚", ["å…³å…³é›é¸ ", "ï¼Œ", "åœ¨æ²³ä¹‹æ´²", "ã€‚"]),
    ("chu_ci", "å¸é«˜é˜³ä¹‹è‹—è£”å…®ï¼Œæœ•çš‡è€ƒæ›°ä¼¯åº¸ã€‚", ["å¸é«˜é˜³ä¹‹è‹—è£”", "å…®", "ï¼Œ", "æœ•çš‡è€ƒæ›°ä¼¯åº¸", "ã€‚"]),
    ("prose", "è½éœä¸å­¤é¹œé½é£ï¼Œç§‹æ°´å…±é•¿å¤©ä¸€è‰²ã€‚", ["è½éœ", "ä¸", "å­¤é¹œ", "é½é£", "ï¼Œ", "ç§‹æ°´", "å…±", "é•¿å¤©", "ä¸€è‰²", "ã€‚"]),
])
def test_text_type_processing(text_type, text_input, expected_tokens):
    """æµ‹è¯•ä¸åŒæ–‡æœ¬ç±»å‹å¤„ç†"""
    tokenizer = ClassicalTokenizer()
    assert tokenizer.tokenize(text_input, text_type=text_type) == expected_tokens

# è¾¹ç•Œæ¡ä»¶æµ‹è¯•
@pytest.mark.parametrize("text_input, expected_tokens", [
    ("", []),
    ("   \n\t", []),
    ("123456", ["123456"]),
    ("@#$%^", ["@", "#", "$", "%", "^"]),
    ("Hello, ä¸–ç•Œ!", ["Hello", ",", " ", "ä¸–ç•Œ", "!"]),
    ("Mixed ä¸­æ–‡ and English 123!", ["Mixed", " ", "ä¸­æ–‡", " ", "and", " ", "English", " ", "123", "!"]),
    ("ç‰¹æ®Šå­—ç¬¦æµ‹è¯•ï¼š\"'\\`~!@#$%^&*()_+-=[]{}|;:,./<>?", ["ç‰¹æ®Šå­—ç¬¦æµ‹è¯•", "ï¼š", "\"", "'", "\\", "`", "~", "!", "@", "#", "$", "%", "^", "&", "*", "(", ")", "_", "+", "-", "=", "[", "]", "{", "}", "|", ";", ":", ",", ".", "/", "<", ">", "?"]),
    ("Emojiæµ‹è¯• ğŸ˜ŠğŸ‘ğŸš€", ["Emojiæµ‹è¯•", " ", "ğŸ˜Š", "ğŸ‘", "ğŸš€"]),
    ("VeryLongStringWithoutSpaces"*100, ["VeryLongStringWithoutSpaces"*100]),
    ("   LeadingAndTrailingSpaces   ", ["LeadingAndTrailingSpaces"]),
])
def test_edge_cases(text_input, expected_tokens):
    """æµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œç‰¹æ®Šå­—ç¬¦"""
    tokenizer = ClassicalTokenizer()
    assert tokenizer.tokenize(text_input) == expected_tokens

def setup_test_environment(dry_run=False):
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒï¼Œåˆ›å»ºå¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶
    
    Args:
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
    
    # å¦‚æœæ˜¯dry_runæ¨¡å¼ï¼Œè®°å½•åˆ°æ—¥å¿—
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    test_dir = Path(tempfile.mkdtemp(prefix="tokenizer_test_"))
    logger.info(f"åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•: {test_dir}")
    
    # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
    dataset_dir = test_dir / "dataset"
    dictionaries_dir = dataset_dir / "dictionaries"
    logs_dir = test_dir / "logs"
    
    for d in [dataset_dir, dictionaries_dir, logs_dir]:
        d.mkdir(exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•è¯å…¸ - dry_runæ¨¡å¼ä¸‹ä½¿ç”¨æ›´å°çš„è¯å…¸
    dict_path = dictionaries_dir / "test_dict.txt"
    with open(dict_path, "w", encoding="utf-8") as f:
        f.write("# æµ‹è¯•è¯å…¸\n")
        f.write("æ˜¥é£ n\n")
        f.write("æ¨æŸ³ n\n")
        f.write("åƒé‡Œ n\n")
        f.write("æ±Ÿå±± n\n")
        f.write("å¦‚ç”» v\n")
        # åœ¨édry_runæ¨¡å¼ä¸‹æ·»åŠ æ›´å¤šè¯æ¡
        if not dry_run:
            f.write("æ˜æœˆ n\n")
            f.write("é•¿å¤© n\n")
            f.write("ç§‹æ°´ n\n")
            f.write("è½éœ n\n")
            f.write("å­¤é¹œ n\n")
    
    # åˆ›å»ºå°å‹æµ‹è¯•è¯­æ–™
    small_corpus_path = dataset_dir / "small_corpus.txt"
    with open(small_corpus_path, "w", encoding="utf-8") as f:
        f.write("æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚\n")
        f.write("åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚\n")
        # åœ¨dry_runæ¨¡å¼ä¸‹å‡å°‘è¯­æ–™æ•°é‡
        if not dry_run:
            f.write("è½éœä¸å­¤é¹œé½é£ï¼Œç§‹æ°´å…±é•¿å¤©ä¸€è‰²ã€‚\n")
    
    # åˆ›å»ºä¸­å‹æµ‹è¯•è¯­æ–™ - dry_runæ¨¡å¼ä¸‹å¤§å¹…å‡å°‘è¯­æ–™é‡
    medium_corpus_path = dataset_dir / "medium_corpus.txt"
    with open(medium_corpus_path, "w", encoding="utf-8") as f:
        poems = [
            "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚",
            "åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚"
        ]
        
        # åœ¨édry_runæ¨¡å¼ä¸‹æ·»åŠ æ›´å¤šè¯—å¥
        if not dry_run:
            poems.extend([
                "è½éœä¸å­¤é¹œé½é£ï¼Œç§‹æ°´å…±é•¿å¤©ä¸€è‰²ã€‚",
                "äººé—²æ¡‚èŠ±è½ï¼Œå¤œé™æ˜¥å±±ç©ºã€‚",
                "æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚",
                "ä¼šå½“å‡Œç»é¡¶ï¼Œä¸€è§ˆä¼—å±±å°ã€‚",
                "ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚",
                "ä¸¤å²¸çŒ¿å£°å•¼ä¸ä½ï¼Œè½»èˆŸå·²è¿‡ä¸‡é‡å±±ã€‚",
                "å­¤å¸†è¿œå½±ç¢§ç©ºå°½ï¼Œå”¯è§é•¿æ±Ÿå¤©é™…æµã€‚",
                "é£æµç›´ä¸‹ä¸‰åƒå°ºï¼Œç–‘æ˜¯é“¶æ²³è½ä¹å¤©ã€‚"
            ])
        
        # dry_runæ¨¡å¼ä¸‹åªé‡å¤å°‘é‡æ¬¡æ•°
        repeat_times = 2 if dry_run else 20
        for _ in range(repeat_times):
            for poem in poems:
                f.write(poem + "\n")
    
    # åˆ›å»ºJSONLæ ¼å¼æµ‹è¯•æ•°æ® - dry_runæ¨¡å¼ä¸‹å‡å°‘æ•°æ®é‡
    jsonl_path = dataset_dir / "test_data.jsonl"
    with open(jsonl_path, "w", encoding="utf-8") as f:
        poems = [
            "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚",
            "åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚"
        ]
        
        # åœ¨édry_runæ¨¡å¼ä¸‹æ·»åŠ æ›´å¤šè¯—å¥
        if not dry_run:
            poems.extend([
                "è½éœä¸å­¤é¹œé½é£ï¼Œç§‹æ°´å…±é•¿å¤©ä¸€è‰²ã€‚",
                "äººé—²æ¡‚èŠ±è½ï¼Œå¤œé™æ˜¥å±±ç©ºã€‚",
                "æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚"
            ])
        
        # dry_runæ¨¡å¼ä¸‹å‡å°‘æ ·æœ¬æ•°é‡
        sample_count = 10 if dry_run else 100
        for i in range(sample_count):
            sample = {
                "id": i,
                "title": f"æµ‹è¯•è¯—è¯{i}",
                "content": random.choice(poems),
                "author": "æµ‹è¯•"
            }
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    
    return test_dir, {
        "dict_path": dict_path,
        "small_corpus_path": small_corpus_path,
        "medium_corpus_path": medium_corpus_path,
        "jsonl_path": jsonl_path
    }

def cleanup_test_environment(test_dir):
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    logger.info(f"æ¸…ç†æµ‹è¯•ç¯å¢ƒ: {test_dir}")
    try:
        shutil.rmtree(test_dir)
    except Exception as e:
        logger.error(f"æ¸…ç†æµ‹è¯•ç¯å¢ƒæ—¶å‡ºé”™: {e}")

def simple_test(dry_run=False):
    """ç®€å•çš„åˆ†è¯å™¨æµ‹è¯•ï¼Œç±»ä¼¼äºåŸå§‹çš„test_tokenizer.py
    
    Args:
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    print("å¼€å§‹ç®€å•æµ‹è¯•åˆ†è¯å™¨...")
    if dry_run:
        print("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
    Path("dataset/dictionaries").mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯å…¸
    dict_path = "dataset/dictionaries/test_dict.txt"
    if not os.path.exists(dict_path):
        with open(dict_path, "w", encoding="utf-8") as f:
            f.write("æ˜¥é£ n\n")
            f.write("æ¨æŸ³ n\n")
            f.write("åƒé‡Œ n\n")
            f.write("æ±Ÿå±± n\n")
            f.write("å¦‚ç”» v\n")
        print(f"åˆ›å»ºæµ‹è¯•è¯å…¸: {dict_path}")
    
    # åˆ›å»ºæµ‹è¯•è¯­æ–™ - dry_runæ¨¡å¼ä¸‹å‡å°‘è¯­æ–™é‡
    corpus_path = "dataset/test_corpus.txt"
    if not os.path.exists(corpus_path):
        with open(corpus_path, "w", encoding="utf-8") as f:
            f.write("æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚\n")
            f.write("åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚\n")
            if not dry_run:
                f.write("è½éœä¸å­¤é¹œé½é£ï¼Œç§‹æ°´å…±é•¿å¤©ä¸€è‰²ã€‚\n")
        print(f"åˆ›å»ºæµ‹è¯•è¯­æ–™: {corpus_path}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("åˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = ClassicalTokenizer(
        vocab_size=1000,  # å°è¯è¡¨ç”¨äºå¿«é€Ÿæµ‹è¯•
        special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
        dictionary_path=dict_path
    )
    
    # è®­ç»ƒåˆ†è¯å™¨
    print("è®­ç»ƒåˆ†è¯å™¨...")
    training_files = [corpus_path]
    
    if dry_run:
        # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä¸å®é™…æ‰§è¡Œå®Œæ•´è®­ç»ƒ
        print("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
        # æ¨¡æ‹Ÿè®­ç»ƒæˆåŠŸ
        success = True
        # ä¸ºtokenizeræ·»åŠ ä¸€äº›åŸºæœ¬çš„è¯æ±‡ï¼Œä»¥ä¾¿åç»­æµ‹è¯•èƒ½å¤Ÿè¿›è¡Œ
        tokenizer.tokenizer = Tokenizer(BPE())
        tokenizer.tokenizer.pre_tokenizer = Whitespace()
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼
        success = tokenizer.train(training_files)
    
    if success:
        print("åˆ†è¯å™¨è®­ç»ƒæˆåŠŸ!")
        
        if not dry_run:
            # ä¿å­˜åˆ†è¯å™¨ - ç›´æ¥ä½¿ç”¨åº•å±‚tokenizerçš„saveæ–¹æ³•
            save_path = "test_tokenizer.json"
            try:
                tokenizer.tokenizer.save(save_path)
                print(f"åˆ†è¯å™¨å·²ä¿å­˜åˆ° {save_path}")
            except Exception as e:
                print(f"ä¿å­˜åˆ†è¯å™¨æ—¶å‡ºé”™: {e}")
        
        # æµ‹è¯•åˆ†è¯
        test_text = "æ˜¥é£åˆç»¿æ±Ÿå—å²¸"
        print(f"\næµ‹è¯•åˆ†è¯: '{test_text}'")
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿåˆ†è¯ç»“æœ
            tokens = ["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"]
            print(f"æ¨¡æ‹Ÿåˆ†è¯ç»“æœ: {tokens}")
        else:
            # å®é™…æ‰§è¡Œåˆ†è¯
            tokens = tokenizer.encode(test_text)
            print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        # æµ‹è¯•è§£ç 
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè§£ç ç»“æœ
            decoded = test_text
            print(f"æ¨¡æ‹Ÿè§£ç ç»“æœ: '{decoded}'")
        else:
            # å®é™…æ‰§è¡Œè§£ç 
            decoded = tokenizer.decode(tokens)
            print(f"è§£ç ç»“æœ: '{decoded}'")
    else:
        print("åˆ†è¯å™¨è®­ç»ƒå¤±è´¥!")
    
    return success

def test_basic_functionality(test_files, dry_run=False):
    """æµ‹è¯•åˆ†è¯å™¨åŸºæœ¬åŠŸèƒ½
    
    Args:
        test_files: æµ‹è¯•æ–‡ä»¶è·¯å¾„å­—å…¸
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("æµ‹è¯•åˆ†è¯å™¨åŸºæœ¬åŠŸèƒ½...")
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    try:
        # åˆå§‹åŒ–åˆ†è¯å™¨
        start_time = time.time()
        tokenizer = ClassicalTokenizer(
            vocab_size=1000,  # å°è¯è¡¨ç”¨äºå¿«é€Ÿæµ‹è¯•
            special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
            dictionary_path=str(test_files["dict_path"])
        )
        init_time = time.time() - start_time
        logger.info(f"åˆ†è¯å™¨åˆå§‹åŒ–è€—æ—¶: {init_time:.4f} ç§’")
        
        # è®­ç»ƒåˆ†è¯å™¨
        start_time = time.time()
        training_files = [str(test_files["small_corpus_path"])]
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä¸å®é™…æ‰§è¡Œå®Œæ•´è®­ç»ƒ
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
            # æ¨¡æ‹Ÿè®­ç»ƒæˆåŠŸ
            success = True
            train_time = 0.01  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            # ä¸ºtokenizeræ·»åŠ ä¸€äº›åŸºæœ¬çš„è¯æ±‡ï¼Œä»¥ä¾¿åç»­æµ‹è¯•èƒ½å¤Ÿè¿›è¡Œ
            tokenizer.tokenizer = Tokenizer(BPE())
            tokenizer.tokenizer.pre_tokenizer = Whitespace()
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            success = tokenizer.train(training_files)
            train_time = time.time() - start_time
        
        if not success:
            logger.error("åˆ†è¯å™¨è®­ç»ƒå¤±è´¥")
            return False
        
        logger.info(f"åˆ†è¯å™¨è®­ç»ƒè€—æ—¶: {train_time:.4f} ç§’")
        
        # æµ‹è¯•åˆ†è¯åŠŸèƒ½
        test_texts = [
            "æ˜¥é£åˆç»¿æ±Ÿå—å²¸",
            "åƒé‡Œæ±Ÿå±±å¦‚ç”»"
        ]
        
        # åœ¨édry_runæ¨¡å¼ä¸‹æ·»åŠ æ›´å¤šæµ‹è¯•æ–‡æœ¬
        if not dry_run:
            test_texts.append("è½éœä¸å­¤é¹œé½é£")
        
        for text in test_texts:
            # ç¼–ç 
            start_time = time.time()
            
            if dry_run:
                # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿåˆ†è¯ç»“æœ
                if text == "æ˜¥é£åˆç»¿æ±Ÿå—å²¸":
                    tokens = ["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"]
                elif text == "åƒé‡Œæ±Ÿå±±å¦‚ç”»":
                    tokens = ["åƒé‡Œ", "æ±Ÿå±±", "å¦‚ç”»"]
                else:
                    tokens = list(text)  # ç®€å•æŒ‰å­—ç¬¦åˆ†è¯
                encode_time = 0.001  # æ¨¡æ‹Ÿç¼–ç æ—¶é—´
            else:
                # å®é™…æ‰§è¡Œåˆ†è¯
                tokens = tokenizer.encode(text)
                encode_time = time.time() - start_time
            
            # è§£ç 
            start_time = time.time()
            
            if dry_run:
                # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè§£ç ç»“æœ
                decoded = text
                decode_time = 0.001  # æ¨¡æ‹Ÿè§£ç æ—¶é—´
            else:
                # å®é™…æ‰§è¡Œè§£ç 
                decoded = tokenizer.decode(tokens)
                decode_time = time.time() - start_time
            
            logger.info(f"æµ‹è¯•æ–‡æœ¬: '{text}'")
            logger.info(f"  {'æ¨¡æ‹Ÿ' if dry_run else ''}ç¼–ç ç»“æœ: {tokens}")
            logger.info(f"  {'æ¨¡æ‹Ÿ' if dry_run else ''}è§£ç ç»“æœ: '{decoded}'")
            logger.info(f"  ç¼–ç è€—æ—¶: {encode_time:.6f} ç§’")
            logger.info(f"  è§£ç è€—æ—¶: {decode_time:.6f} ç§’")
            
            # éªŒè¯è§£ç ç»“æœæ˜¯å¦ä¸åŸæ–‡æœ¬ç›¸ä¼¼
            similarity = len(set(text) & set(decoded)) / len(set(text) | set(decoded))
            logger.info(f"  æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity:.2f}")
            assert similarity > 0.5, f"è§£ç ç»“æœä¸åŸæ–‡æœ¬ç›¸ä¼¼åº¦è¿‡ä½: {similarity:.2f}"
        
        logger.info("åˆ†è¯å™¨åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        logger.error(f"åˆ†è¯å™¨åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance(test_files, dry_run=False):
    """æµ‹è¯•åˆ†è¯å™¨æ€§èƒ½
    
    Args:
        test_files: æµ‹è¯•æ–‡ä»¶è·¯å¾„å­—å…¸
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("æµ‹è¯•åˆ†è¯å™¨æ€§èƒ½...")
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    try:
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = ClassicalTokenizer(
            vocab_size=1000,
            special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
            dictionary_path=str(test_files["dict_path"])
        )
        
        # è®­ç»ƒåˆ†è¯å™¨ - ä½¿ç”¨ä¸­å‹è¯­æ–™
        logger.info("ä½¿ç”¨ä¸­å‹è¯­æ–™è®­ç»ƒåˆ†è¯å™¨...")
        start_time = time.time()
        training_files = [str(test_files["medium_corpus_path"])]
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä¸å®é™…æ‰§è¡Œå®Œæ•´è®­ç»ƒ
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
            # æ¨¡æ‹Ÿè®­ç»ƒæˆåŠŸ
            success = True
            train_time = 0.01  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            # ä¸ºtokenizeræ·»åŠ ä¸€äº›åŸºæœ¬çš„è¯æ±‡ï¼Œä»¥ä¾¿åç»­æµ‹è¯•èƒ½å¤Ÿè¿›è¡Œ
            tokenizer.tokenizer = Tokenizer(BPE())
            tokenizer.tokenizer.pre_tokenizer = Whitespace()
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            success = tokenizer.train(training_files)
            train_time = time.time() - start_time
        
        if not success:
            logger.error("åˆ†è¯å™¨è®­ç»ƒå¤±è´¥")
            return False
        
        logger.info(f"ä¸­å‹è¯­æ–™è®­ç»ƒè€—æ—¶: {train_time:.4f} ç§’")
        
        # æ€§èƒ½æµ‹è¯• - å•ä¸ªæ–‡æœ¬åˆ†è¯
        test_text = "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚" * (2 if dry_run else 10)
        
        # é¢„çƒ­
        if dry_run:
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿé¢„çƒ­è¿‡ç¨‹")
        else:
            for _ in range(5):
                tokenizer.encode(test_text)
        
        # æµ‹é‡å•ä¸ªæ–‡æœ¬åˆ†è¯æ€§èƒ½
        iterations = 10 if dry_run else 100
        encode_times = []
        
        for i in range(iterations):
            start_time = time.time()
            if dry_run:
                # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿåˆ†è¯ç»“æœ
                tokens = ["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"] * 10
                # æ¨¡æ‹Ÿä¸€ä¸ªåˆç†çš„ç¼–ç æ—¶é—´
                time.sleep(0.001)
                encode_time = 0.001
            else:
                # å®é™…æ‰§è¡Œåˆ†è¯
                tokens = tokenizer.encode(test_text)
                encode_time = time.time() - start_time
            encode_times.append(encode_time)
        
        avg_encode_time = sum(encode_times) / len(encode_times)
        min_encode_time = min(encode_times)
        max_encode_time = max(encode_times)
        
        logger.info(f"å•ä¸ªæ–‡æœ¬åˆ†è¯æ€§èƒ½ ({iterations} æ¬¡è¿­ä»£):")
        logger.info(f"  å¹³å‡è€—æ—¶: {avg_encode_time:.6f} ç§’")
        logger.info(f"  æœ€å°è€—æ—¶: {min_encode_time:.6f} ç§’")
        logger.info(f"  æœ€å¤§è€—æ—¶: {max_encode_time:.6f} ç§’")
        
        # æ€§èƒ½æµ‹è¯• - æ‰¹é‡åˆ†è¯
        test_texts = [test_text] * (3 if dry_run else 10)
        
        # é¢„çƒ­
        if not dry_run and hasattr(tokenizer, 'batch_tokenize'):
            tokenizer.batch_tokenize(test_texts[:5])
        else:
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿæ‰¹é‡é¢„çƒ­è¿‡ç¨‹")
        
        # æµ‹é‡æ‰¹é‡åˆ†è¯æ€§èƒ½
        batch_iterations = 3 if dry_run else 10
        batch_times = []
        
        for i in range(batch_iterations):
            start_time = time.time()
            if dry_run:
                # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿæ‰¹é‡åˆ†è¯ç»“æœ
                batch_tokens = [["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"] * 10] * len(test_texts)
                # æ¨¡æ‹Ÿä¸€ä¸ªåˆç†çš„æ‰¹å¤„ç†æ—¶é—´
                time.sleep(0.002)
                batch_time = 0.002
            else:
                if hasattr(tokenizer, 'batch_tokenize'):
                    batch_tokens = tokenizer.batch_tokenize(test_texts)
                else:
                    batch_tokens = [tokenizer.tokenize(text) for text in test_texts]
                batch_time = time.time() - start_time
            batch_times.append(batch_time)
        
        avg_batch_time = sum(batch_times) / len(batch_times)
        min_batch_time = min(batch_times)
        max_batch_time = max(batch_times)
        
        logger.info(f"æ‰¹é‡åˆ†è¯æ€§èƒ½ ({batch_iterations} æ¬¡è¿­ä»£, æ¯æ‰¹ {len(test_texts)} ä¸ªæ–‡æœ¬):")
        logger.info(f"  å¹³å‡è€—æ—¶: {avg_batch_time:.6f} ç§’")
        logger.info(f"  æœ€å°è€—æ—¶: {min_batch_time:.6f} ç§’")
        logger.info(f"  æœ€å¤§è€—æ—¶: {max_batch_time:.6f} ç§’")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        single_total_time = avg_encode_time * len(test_texts)
        speedup = single_total_time / avg_batch_time if avg_batch_time > 0 else float('inf')
        logger.info(f"æ‰¹é‡å¤„ç†åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        # æµ‹è¯•ç¼“å­˜æ•ˆæœ
        logger.info("æµ‹è¯•ç¼“å­˜æ•ˆæœ...")
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿç¼“å­˜æ•ˆæœ
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿç¼“å­˜æ•ˆæœæµ‹è¯•")
            no_cache_time = 0.002
            with_cache_time = 0.0005
            cache_speedup = 4.0  # æ¨¡æ‹Ÿä¸€ä¸ªåˆç†çš„ç¼“å­˜åŠ é€Ÿæ¯”
        else:
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ - æ— ç¼“å­˜
            start_time = time.time()
            tokenizer.encode(test_text)
            no_cache_time = time.time() - start_time
            
            # ç¬¬äºŒæ¬¡è°ƒç”¨ - æœ‰ç¼“å­˜
            start_time = time.time()
            tokenizer.encode(test_text)
            with_cache_time = time.time() - start_time
            
            cache_speedup = no_cache_time / with_cache_time if with_cache_time > 0 else float('inf')
        logger.info(f"æ— ç¼“å­˜è€—æ—¶: {no_cache_time:.6f} ç§’")
        logger.info(f"æœ‰ç¼“å­˜è€—æ—¶: {with_cache_time:.6f} ç§’")
        logger.info(f"ç¼“å­˜åŠ é€Ÿæ¯”: {cache_speedup:.2f}x")
        
        logger.info("åˆ†è¯å™¨æ€§èƒ½æµ‹è¯•é€šè¿‡")
        return True, {
            "avg_encode_time": avg_encode_time,
            "avg_batch_time": avg_batch_time,
            "batch_speedup": speedup,
            "cache_speedup": cache_speedup
        }
    except Exception as e:
        logger.error(f"åˆ†è¯å™¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_edge_cases(test_files, dry_run=False):
    """æµ‹è¯•åˆ†è¯å™¨è¾¹ç•Œæ¡ä»¶
    
    Args:
        test_files: æµ‹è¯•æ–‡ä»¶è·¯å¾„å­—å…¸
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("æµ‹è¯•åˆ†è¯å™¨è¾¹ç•Œæ¡ä»¶...")
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    try:
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = ClassicalTokenizer(
            vocab_size=1000,
            special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
            dictionary_path=str(test_files["dict_path"])
        )
        
        # è®­ç»ƒåˆ†è¯å™¨
        training_files = [str(test_files["small_corpus_path"])]
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œä¸å®é™…æ‰§è¡Œå®Œæ•´è®­ç»ƒ
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
            # æ¨¡æ‹Ÿè®­ç»ƒæˆåŠŸ
            success = True
            # ä¸ºtokenizeræ·»åŠ ä¸€äº›åŸºæœ¬çš„è¯æ±‡ï¼Œä»¥ä¾¿åç»­æµ‹è¯•èƒ½å¤Ÿè¿›è¡Œ
            tokenizer.tokenizer = Tokenizer(BPE())
            tokenizer.tokenizer.pre_tokenizer = Whitespace()
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            success = tokenizer.train(training_files)
        
        if not success:
            logger.error("åˆ†è¯å™¨è®­ç»ƒå¤±è´¥")
            return False
        
        # æµ‹è¯•è¾¹ç•Œæ¡ä»¶
        edge_cases = [
            "",  # ç©ºå­—ç¬¦ä¸²
            "a",  # å•ä¸ªå­—ç¬¦
            "æ˜¥",  # å•ä¸ªä¸­æ–‡å­—ç¬¦
            "æ˜¥é£" * 1000,  # è¶…é•¿æ–‡æœ¬
            "12345",  # çº¯æ•°å­—
            "!@#$%^",  # ç‰¹æ®Šå­—ç¬¦
            "\n\t\r",  # æ§åˆ¶å­—ç¬¦
            "æ˜¥é£abc123",  # æ··åˆå­—ç¬¦
            "æ˜¥é£\nåˆç»¿\tæ±Ÿå—",  # åŒ…å«æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
            "\u3000\u3000æ˜¥é£",  # åŒ…å«å…¨è§’ç©ºæ ¼
        ]
        
        for i, case in enumerate(edge_cases):
            logger.info(f"æµ‹è¯•è¾¹ç•Œæ¡ä»¶ #{i+1}: '{case[:20]}{'...' if len(case) > 20 else ''}'")
            
            try:
                if dry_run:
                    # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿåˆ†è¯å’Œè§£ç ç»“æœ
                    if case == "":
                        tokens = []
                    elif case == "a":
                        tokens = ["a"]
                    elif case == "æ˜¥":
                        tokens = ["æ˜¥"]
                    elif "æ˜¥é£" in case and len(case) > 100:
                        tokens = ["æ˜¥é£"] * 50
                    elif case == "12345":
                        tokens = ["1", "2", "3", "4", "5"]
                    elif case == "!@#$%^":
                        tokens = ["!", "@", "#", "$", "%", "^"]
                    elif case == "\n\t\r":
                        tokens = ["\n", "\t", "\r"]
                    elif "æ˜¥é£abc123" in case:
                        tokens = ["æ˜¥é£", "a", "b", "c", "1", "2", "3"]
                    else:
                        tokens = list(case[:10])  # ç®€å•æŒ‰å­—ç¬¦åˆ†è¯ï¼Œæœ€å¤š10ä¸ª
                    
                    # æ¨¡æ‹Ÿè§£ç ç»“æœ
                    decoded = case[:20] if len(case) > 0 else ""
                    
                    logger.info(f"  æ¨¡æ‹Ÿç¼–ç ç»“æœ: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
                    logger.info(f"  æ¨¡æ‹Ÿè§£ç ç»“æœ: '{decoded[:20]}{'...' if len(decoded) > 20 else ''}'")
                    logger.info(f"  å¤„ç†æˆåŠŸ (æ¨¡æ‹Ÿ)")
                else:
                    # å®é™…æ‰§è¡Œåˆ†è¯å’Œè§£ç 
                    # ç¼–ç 
                    tokens = tokenizer.encode(case)
                    
                    # è§£ç 
                    decoded = tokenizer.decode(tokens)
                    
                    logger.info(f"  ç¼–ç ç»“æœ: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
                    logger.info(f"  è§£ç ç»“æœ: '{decoded[:20]}{'...' if len(decoded) > 20 else ''}'")
                    logger.info(f"  å¤„ç†æˆåŠŸ")
            except Exception as e:
                logger.error(f"  å¤„ç†å¤±è´¥: {e}")
                return False
        
        # æµ‹è¯•é”™è¯¯å¤„ç†
        error_cases = [
            (None, "ç©ºå€¼"),
            (123, "éå­—ç¬¦ä¸²ç±»å‹"),
            (["æ˜¥é£", "åˆç»¿"], "åˆ—è¡¨ç±»å‹"),
            ({"text": "æ˜¥é£"}, "å­—å…¸ç±»å‹")
        ]
        
        for case, desc in error_cases:
            logger.info(f"æµ‹è¯•é”™è¯¯å¤„ç†: {desc}")
            
            if dry_run:
                # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿé”™è¯¯å¤„ç†
                logger.info(f"  æ¨¡æ‹Ÿé¢„æœŸçš„é”™è¯¯å¤„ç†: è¾“å…¥ç±»å‹é”™è¯¯ - {desc}")
            else:
                try:
                    tokens = tokenizer.encode(case)
                    logger.warning(f"  é¢„æœŸå¤±è´¥ä½†æˆåŠŸå¤„ç†: {tokens}")
                except Exception as e:
                    logger.info(f"  é¢„æœŸçš„é”™è¯¯å¤„ç†: {e}")
        
        logger.info("åˆ†è¯å™¨è¾¹ç•Œæ¡ä»¶æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        logger.error(f"åˆ†è¯å™¨è¾¹ç•Œæ¡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_jsonl_processing(test_files, dry_run=False):
    """æµ‹è¯•JSONLæ–‡ä»¶å¤„ç†
    
    Args:
        test_files: æµ‹è¯•æ–‡ä»¶è·¯å¾„å­—å…¸
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("æµ‹è¯•JSONLæ–‡ä»¶å¤„ç†...")
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    try:
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = ClassicalTokenizer(
            vocab_size=1000,
            special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
            dictionary_path=str(test_files["dict_path"])
        )
        
        # ä»JSONLæ–‡ä»¶æå–æ–‡æœ¬
        start_time = time.time()
        jsonl_files = [str(test_files["jsonl_path"])]
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹ŸJSONLæ–‡ä»¶å¤„ç†
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹ŸJSONLæ–‡ä»¶å¤„ç†è¿‡ç¨‹")
            # æ¨¡æ‹Ÿæå–çš„æ–‡æœ¬è¡Œ
            text_lines = [
                "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚",
                "åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚"
            ] * 5  # æ¨¡æ‹Ÿ10è¡Œæ–‡æœ¬
            extract_time = 0.01  # æ¨¡æ‹Ÿæå–æ—¶é—´
        else:
            # å®é™…æ‰§è¡ŒJSONLæ–‡ä»¶å¤„ç†
            text_lines = tokenizer.extract_text_from_jsonl(jsonl_files)
            extract_time = time.time() - start_time
        
        logger.info(f"ä»JSONLæ–‡ä»¶æå–æ–‡æœ¬è€—æ—¶: {extract_time:.4f} ç§’")
        logger.info(f"æå–çš„æ–‡æœ¬è¡Œæ•°: {len(text_lines)}")
        
        # éªŒè¯æå–çš„æ–‡æœ¬
        assert len(text_lines) > 0, "æœªä»JSONLæ–‡ä»¶æå–åˆ°æ–‡æœ¬"
        
        # ä½¿ç”¨æå–çš„æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨
        start_time = time.time()
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            logger.info("è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹è€Œä¸æ‰§è¡Œå®Œæ•´è®­ç»ƒ")
            # æ¨¡æ‹Ÿè®­ç»ƒæˆåŠŸ
            success = True
            train_time = 0.01  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            # ä¸ºtokenizeræ·»åŠ ä¸€äº›åŸºæœ¬çš„è¯æ±‡ï¼Œä»¥ä¾¿åç»­æµ‹è¯•èƒ½å¤Ÿè¿›è¡Œ
            tokenizer.tokenizer = Tokenizer(BPE())
            tokenizer.tokenizer.pre_tokenizer = Whitespace()
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            success = tokenizer.train(training_files=None)  # ä½¿ç”¨æå–çš„æ–‡æœ¬è®­ç»ƒ
            train_time = time.time() - start_time
        
        if not success:
            logger.error("ä½¿ç”¨JSONLæå–çš„æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨å¤±è´¥")
            return False
        
        logger.info(f"ä½¿ç”¨JSONLæå–çš„æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨è€—æ—¶: {train_time:.4f} ç§’")
        
        # æµ‹è¯•åˆ†è¯ç»“æœ
        test_text = "æ˜¥é£åˆç»¿æ±Ÿå—å²¸"
        
        if dry_run:
            # åœ¨dry_runæ¨¡å¼ä¸‹æ¨¡æ‹Ÿåˆ†è¯ç»“æœ
            tokens = ["æ˜¥é£", "åˆ", "ç»¿", "æ±Ÿå—", "å²¸"]
            decoded = test_text
            logger.info(f"æµ‹è¯•æ–‡æœ¬: '{test_text}'")
            logger.info(f"  æ¨¡æ‹Ÿç¼–ç ç»“æœ: {tokens}")
            logger.info(f"  æ¨¡æ‹Ÿè§£ç ç»“æœ: '{decoded}'")
        else:
            # å®é™…æ‰§è¡Œåˆ†è¯å’Œè§£ç 
            tokens = tokenizer.encode(test_text)
            decoded = tokenizer.decode(tokens)
            logger.info(f"æµ‹è¯•æ–‡æœ¬: '{test_text}'")
            logger.info(f"  ç¼–ç ç»“æœ: {tokens}")
            logger.info(f"  è§£ç ç»“æœ: '{decoded}'")
        
        logger.info("JSONLæ–‡ä»¶å¤„ç†æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        logger.error(f"JSONLæ–‡ä»¶å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def plot_performance_results(performance_data, save_dir):
    """ç»˜åˆ¶æ€§èƒ½æµ‹è¯•ç»“æœå›¾è¡¨"""
    if not performance_data:
        logger.warning("æ²¡æœ‰æ€§èƒ½æ•°æ®å¯ä¾›ç»˜å›¾")
        return
    
    try:
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True, parents=True)
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8-darkgrid')
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºå­å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('åˆ†è¯å™¨æ€§èƒ½æµ‹è¯•ç»“æœ', fontsize=16)
        
        # ç»˜åˆ¶åŠ é€Ÿæ¯”å¯¹æ¯”å›¾
        labels = ['æ‰¹å¤„ç†åŠ é€Ÿæ¯”', 'ç¼“å­˜åŠ é€Ÿæ¯”']
        values = [performance_data['batch_speedup'], performance_data['cache_speedup']]
        colors = ['#3498db', '#2ecc71']
        
        ax1.bar(labels, values, color=colors)
        ax1.set_title('åŠ é€Ÿæ¯”å¯¹æ¯”')
        ax1.set_ylabel('åŠ é€Ÿæ¯” (xå€)')
        ax1.grid(True, linestyle='--', alpha=0.7)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(values):
            ax1.text(i, v + 0.1, f'{v:.2f}x', ha='center')
        
        # ç»˜åˆ¶å¤„ç†æ—¶é—´å¯¹æ¯”å›¾
        labels = ['å•ä¸ªæ–‡æœ¬å¹³å‡æ—¶é—´', 'æ‰¹å¤„ç†å¹³å‡æ—¶é—´']
        values = [performance_data['avg_encode_time'], performance_data['avg_batch_time'] / 10]  # é™¤ä»¥10è·å–æ¯ä¸ªæ–‡æœ¬çš„å¹³å‡æ—¶é—´
        
        ax2.bar(labels, values, color=colors)
        ax2.set_title('å¤„ç†æ—¶é—´å¯¹æ¯” (æ¯ä¸ªæ–‡æœ¬)')
        ax2.set_ylabel('æ—¶é—´ (ç§’)')
        ax2.grid(True, linestyle='--', alpha=0.7)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(values):
            ax2.text(i, v + 0.0001, f'{v:.6f}s', ha='center')
        
        # æ·»åŠ æµ‹è¯•ä¿¡æ¯
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        fig.text(0.5, 0.01, f'æµ‹è¯•æ—¶é—´: {timestamp}', ha='center', fontsize=10)
        
        # ä¿å­˜å›¾è¡¨
        plot_path = save_dir / f'tokenizer_performance_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(plot_path, dpi=300)
        plt.close()
        
        logger.info(f"æ€§èƒ½æµ‹è¯•ç»“æœå›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
        return plot_path
    except Exception as e:
        logger.error(f"ç»˜åˆ¶æ€§èƒ½æµ‹è¯•ç»“æœå›¾è¡¨å¤±è´¥: {e}")
        return None

def collect_performance_data(dry_run=False):
    """æ”¶é›†åˆ†è¯å™¨æ€§èƒ½æ•°æ®
    
    Args:
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("å¼€å§‹æ”¶é›†åˆ†è¯å™¨æ€§èƒ½æ•°æ®...")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    test_dir, test_files = setup_test_environment(dry_run=dry_run)
    
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    try:
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = ClassicalTokenizer(
            vocab_size=1000,
            special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]", "[SEP]", "[MASK]"],
            dictionary_path=str(test_files["dict_path"])
        )
        
        # è®­ç»ƒåˆ†è¯å™¨
        training_files = [str(test_files["medium_corpus_path"])]
        success = tokenizer.train(training_files)
        
        if not success:
            logger.error("åˆ†è¯å™¨è®­ç»ƒå¤±è´¥")
            return None
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬ - ä¸åŒé•¿åº¦
        test_texts = {
            "çŸ­æ–‡æœ¬": "æ˜¥é£åˆç»¿æ±Ÿå—å²¸",
            "ä¸­æ–‡æœ¬": "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚åƒé‡Œæ±Ÿå±±å¦‚ç”»ï¼Œé£æ™¯ç‹¬å¥½ã€‚",
            "é•¿æ–‡æœ¬": "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚" * 10,
            "è¶…é•¿æ–‡æœ¬": "æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚" * 100
        }
        
        # æ”¶é›†ä¸åŒæ–‡æœ¬é•¿åº¦çš„æ€§èƒ½æ•°æ®
        performance_data = []
        
        for name, text in test_texts.items():
            logger.info(f"æµ‹è¯• {name} (é•¿åº¦: {len(text)})...")
            
            # é¢„çƒ­
            for _ in range(3):
                tokenizer.encode(text)
            
            # æµ‹é‡ç¼–ç æ—¶é—´
            iterations = 10
            encode_times = []
            
            for _ in range(iterations):
                start_time = time.time()
                tokens = tokenizer.encode(text)
                encode_time = time.time() - start_time
                encode_times.append(encode_time)
            
            avg_time = sum(encode_times) / len(encode_times)
            min_time = min(encode_times)
            max_time = max(encode_times)
            
            # è®°å½•æ€§èƒ½æ•°æ®
            performance_data.append({
                "name": name,
                "length": len(text),
                "avg_time": avg_time,
                "min_time": min_time,
                "max_time": max_time,
                "tokens_per_second": len(text) / avg_time if avg_time > 0 else float('inf')
            })
            
            logger.info(f"  å¹³å‡è€—æ—¶: {avg_time:.6f} ç§’")
            logger.info(f"  æœ€å°è€—æ—¶: {min_time:.6f} ç§’")
            logger.info(f"  æœ€å¤§è€—æ—¶: {max_time:.6f} ç§’")
            logger.info(f"  å¤„ç†é€Ÿåº¦: {len(text) / avg_time:.2f} å­—ç¬¦/ç§’")
        
        # è¾“å‡ºæ€§èƒ½æ•°æ®
        logger.info("\næ€§èƒ½æ•°æ®æ±‡æ€»:")
        for data in performance_data:
            logger.info(f"{data['name']} (é•¿åº¦: {data['length']}):")
            logger.info(f"  å¹³å‡è€—æ—¶: {data['avg_time']:.6f} ç§’")
            logger.info(f"  å¤„ç†é€Ÿåº¦: {data['tokens_per_second']:.2f} å­—ç¬¦/ç§’")
        
        # ç»˜åˆ¶æ€§èƒ½å›¾è¡¨
        plot_path = test_dir / "logs" / f"tokenizer_performance_by_length_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        
        try:
            plt.figure(figsize=(12, 10))
            
            # ç»˜åˆ¶å¤„ç†æ—¶é—´ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»
            plt.subplot(2, 1, 1)
            lengths = [data["length"] for data in performance_data]
            times = [data["avg_time"] for data in performance_data]
            plt.plot(lengths, times, 'o-', linewidth=2, markersize=8, color='#3498db')
            plt.title('å¤„ç†æ—¶é—´ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»')
            plt.xlabel('æ–‡æœ¬é•¿åº¦ (å­—ç¬¦æ•°)')
            plt.ylabel('å¤„ç†æ—¶é—´ (ç§’)')
            plt.grid(True, linestyle='--', alpha=0.7)
            
            # æ·»åŠ æ•°æ®ç‚¹æ ‡ç­¾
            for i, (length, time) in enumerate(zip(lengths, times)):
                plt.annotate(f'{time:.6f}s', (length, time), 
                            textcoords="offset points", 
                            xytext=(0, 10), 
                            ha='center')
            
            # ç»˜åˆ¶å¤„ç†é€Ÿåº¦ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»
            plt.subplot(2, 1, 2)
            speeds = [data["tokens_per_second"] for data in performance_data]
            plt.plot(lengths, speeds, 'o-', linewidth=2, markersize=8, color='#2ecc71')
            plt.title('å¤„ç†é€Ÿåº¦ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»')
            plt.xlabel('æ–‡æœ¬é•¿åº¦ (å­—ç¬¦æ•°)')
            plt.ylabel('å¤„ç†é€Ÿåº¦ (å­—ç¬¦/ç§’)')
            plt.grid(True, linestyle='--', alpha=0.7)
            
            # æ·»åŠ æ•°æ®ç‚¹æ ‡ç­¾
            for i, (length, speed) in enumerate(zip(lengths, speeds)):
                plt.annotate(f'{speed:.2f}', (length, speed), 
                            textcoords="offset points", 
                            xytext=(0, 10), 
                            ha='center')
            
            plt.tight_layout()
            plt.savefig(plot_path, dpi=300)
            plt.close()
            
            logger.info(f"æ€§èƒ½æ•°æ®å›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
        except Exception as e:
            logger.error(f"ç»˜åˆ¶æ€§èƒ½æ•°æ®å›¾è¡¨å¤±è´¥: {e}")
        
        return performance_data
    except Exception as e:
        logger.error(f"æ”¶é›†æ€§èƒ½æ•°æ®æ—¶å‡ºé”™: {e}")
        return None
    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        cleanup_test_environment(test_dir)

def run_all_tests(dry_run=False):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•
    
    Args:
        dry_run: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    """
    logger.info("å¼€å§‹è¿è¡Œæ‰€æœ‰åˆ†è¯å™¨æµ‹è¯•...")
    if dry_run:
        logger.info("ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼(dry_run)ï¼Œå°†å‡å°‘èµ„æºæ¶ˆè€—")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    test_dir, test_files = setup_test_environment(dry_run=dry_run)
    
    tests = [
        ("åŸºæœ¬åŠŸèƒ½", lambda: test_basic_functionality(test_files, dry_run=dry_run)),
        ("æ€§èƒ½æµ‹è¯•", lambda: test_performance(test_files, dry_run=dry_run)),
        ("è¾¹ç•Œæ¡ä»¶", lambda: test_edge_cases(test_files, dry_run=dry_run)),
        ("JSONLå¤„ç†", lambda: test_jsonl_processing(test_files, dry_run=dry_run))
    ]
    
    results = {}
    all_passed = True
    performance_data = None
    
    for name, test_func in tests:
        logger.info(f"\n{'=' * 50}\næµ‹è¯• {name}\n{'=' * 50}")
        start_time = time.time()
        try:
            if name == "æ€§èƒ½æµ‹è¯•":
                passed, perf_data = test_func()
                if passed and perf_data:
                    performance_data = perf_data
            else:
                passed = test_func()
                
            duration = time.time() - start_time
            status = "é€šè¿‡" if passed else "å¤±è´¥"
            results[name] = {
                "status": status,
                "duration": duration
            }
            logger.info(f"{name} æµ‹è¯•{status}ï¼Œè€—æ—¶: {duration:.4f} ç§’")
            if not passed:
                all_passed = False
        except Exception as e:
            logger.error(f"{name} æµ‹è¯•å‡ºé”™: {e}")
            results[name] = {
                "status": "é”™è¯¯",
                "error": str(e)
            }
            all_passed = False
    
    # ç»˜åˆ¶æ€§èƒ½æµ‹è¯•ç»“æœå›¾è¡¨
    if performance_data:
        plot_performance_results(performance_data, test_dir / "logs")
    
    # æ±‡æ€»æµ‹è¯•ç»“æœ
    logger.info("\n" + "=" * 50)
    logger.info("åˆ†è¯å™¨æµ‹è¯•æ±‡æ€»:")
    for name, result in results.items():
        status = result["status"]
        if status == "é€šè¿‡":
            logger.info(f"âœ… {name}: {status}, è€—æ—¶: {result['duration']:.4f} ç§’")
        else:
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
            logger.info(f"âŒ {name}: {status}, é”™è¯¯: {error_msg}")
    
    overall = "å…¨éƒ¨é€šè¿‡" if all_passed else "éƒ¨åˆ†å¤±è´¥"
    logger.info(f"æµ‹è¯•ç»“æœ: {overall}")
    logger.info("=" * 50)
    
    # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    cleanup_test_environment(test_dir)
    
    return all_passed

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†è¯å™¨æµ‹è¯•è„šæœ¬")
    parser.add_argument("--simple", action="store_true", help="è¿è¡Œç®€å•æµ‹è¯•ï¼ˆåŸå§‹test_tokenizer.pyåŠŸèƒ½ï¼‰")
    parser.add_argument("--all", action="store_true", help="è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    parser.add_argument("--basic", action="store_true", help="æµ‹è¯•åŸºæœ¬åŠŸèƒ½")
    parser.add_argument("--perf", action="store_true", help="æµ‹è¯•æ€§èƒ½")
    parser.add_argument("--edge", action="store_true", help="æµ‹è¯•è¾¹ç•Œæ¡ä»¶")
    parser.add_argument("--jsonl", action="store_true", help="æµ‹è¯•JSONLå¤„ç†")
    parser.add_argument("--collect", action="store_true", help="æ”¶é›†æ€§èƒ½æ•°æ®")
    parser.add_argument("--dry-run", action="store_true", default=True, help="ä½¿ç”¨è½»é‡çº§æµ‹è¯•æ¨¡å¼ï¼Œå‡å°‘èµ„æºæ¶ˆè€—ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--full", action="store_false", dest="dry_run", help="ä½¿ç”¨å®Œæ•´æµ‹è¯•æ¨¡å¼ï¼Œæ¶ˆè€—æ›´å¤šèµ„æº")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°ï¼Œåˆ™è¿è¡Œç®€å•æµ‹è¯•
    if not any([args.simple, args.all, args.basic, args.perf, args.edge, args.jsonl, args.collect]):
        args.simple = True
    
    # è·å–dry_runå‚æ•°
    dry_run = args.dry_run
    
    if args.simple:
        simple_test(dry_run=dry_run)
    elif args.all:
        run_all_tests(dry_run=dry_run)
    else:
        test_dir, test_files = setup_test_environment(dry_run=dry_run)
        try:
            if args.basic:
                test_basic_functionality(test_files, dry_run=dry_run)
            if args.perf:
                test_performance(test_files, dry_run=dry_run)
            if args.edge:
                test_edge_cases(test_files, dry_run=dry_run)
            if args.jsonl:
                test_jsonl_processing(test_files, dry_run=dry_run)
            if args.collect:
                collect_performance_data(dry_run=dry_run)
        finally:
            cleanup_test_environment(test_dir)

if __name__ == "__main__":
    main()