import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
import argparse
import signal
import time
import json
import logging
import glob
import math
from tqdm import tqdm
from datetime import datetime
from torch.cuda.amp import autocast, GradScaler
from torch.utils.checkpoint import checkpoint
from pathlib import Path
import shutil
from termcolor import colored
import psutil

# è®¾ç½®ç‰ˆæœ¬å·
VERSION = "0.8.5"

# ================= å·¥å…·å‡½æ•° =================

def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
    if torch.cuda.is_available():
        gpu_memory = {
            'total': torch.cuda.get_device_properties(0).total_memory,
            'reserved': torch.cuda.memory_reserved(0),
            'allocated': torch.cuda.memory_allocated(0)
        }
        return gpu_memory
    return None

def format_memory_size(size_bytes):
    """æ ¼å¼åŒ–å†…å­˜å¤§å°æ˜¾ç¤º"""
    if size_bytes is None:
        return "æœªçŸ¥"
    
    size_kb = size_bytes / 1024
    if size_kb < 1024:
        return f"{size_kb:.2f} KB"
    
    size_mb = size_kb / 1024
    if size_mb < 1024:
        return f"{size_mb:.2f} MB"
    
    size_gb = size_mb / 1024
    return f"{size_gb:.2f} GB"

def format_time(seconds):
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ›´æ˜“è¯»çš„æ—¶é—´æ ¼å¼"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        minutes = (seconds % 3600) / 60
        return f"{hours:.1f}å°æ—¶{minutes:.1f}åˆ†é’Ÿ"

def plot_training_stats(stats, save_dir):
    """ç»˜åˆ¶è®­ç»ƒç»Ÿè®¡å›¾è¡¨"""
    plt.style.use('seaborn-v0_8-darkgrid')
    
    # è®¾ç½®å­—ä½“å’Œé¢œè‰²ä¸»é¢˜
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['font.size'] = 12
    
    # åˆ›å»ºä¸€ä¸ª2x2çš„å­å›¾å¸ƒå±€
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.patch.set_facecolor('#F5F5F5')
    
    # ä¸»é¢˜é¢œè‰²
    colors = {
        'loss': '#3498db',     # è“è‰²
        'lr': '#2ecc71',       # ç»¿è‰²
        'time': '#e74c3c',     # çº¢è‰²
        'memory': '#9b59b6'    # ç´«è‰²
    }
    
    # 1. æŸå¤±æ›²çº¿
    epochs = range(1, len(stats['losses']) + 1)
    ax1.plot(epochs, stats['losses'], 'o-', linewidth=2, markersize=8, 
             color=colors['loss'], label='è®­ç»ƒæŸå¤±')
    ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontweight='bold', fontsize=14)
    ax1.set_xlabel('è½®æ¬¡', fontsize=12)
    ax1.set_ylabel('æŸå¤±å€¼', fontsize=12)
    ax1.grid(True, linestyle='--', alpha=0.7)
    
    # æ·»åŠ æœ€å°å€¼æ ‡è®°
    min_loss = min(stats['losses'])
    min_idx = stats['losses'].index(min_loss)
    ax1.annotate(f'æœ€å°å€¼: {min_loss:.4f}',
                xy=(min_idx + 1, min_loss), 
                xytext=(min_idx + 1 + 0.5, min_loss + 0.5),
                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),
                fontsize=11)
    
    ax1.legend(loc='upper right')
    
    # 2. å­¦ä¹ ç‡å˜åŒ–
    steps_per_epoch = len(stats['learning_rates']) // len(epochs)
    if steps_per_epoch > 1:
        # å¦‚æœæ¯ä¸ªepochæœ‰å¤šä¸ªå­¦ä¹ ç‡è®°å½•ï¼Œåˆ›å»ºè¯¦ç»†çš„å­¦ä¹ ç‡æ›²çº¿
        all_steps = range(1, len(stats['learning_rates']) + 1)
        ax2.plot(all_steps, stats['learning_rates'], '-', linewidth=2, 
                color=colors['lr'], label='å­¦ä¹ ç‡')
        
        # æ ‡è®°é¢„çƒ­é˜¶æ®µ
        warmup_steps = int(0.1 * len(stats['learning_rates']))
        if warmup_steps > 0:
            ax2.axvspan(1, warmup_steps, alpha=0.2, color='yellow')
            ax2.text(warmup_steps/2, min(stats['learning_rates']), 'é¢„çƒ­é˜¶æ®µ', 
                    ha='center', va='bottom', fontsize=10)
        
        ax2.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)
    else:
        # æŒ‰è½®æ¬¡æ˜¾ç¤ºå­¦ä¹ ç‡
        ax2.plot(epochs, stats['learning_rates'], 'o-', linewidth=2, markersize=8, 
                color=colors['lr'], label='å­¦ä¹ ç‡')
        ax2.set_xlabel('è½®æ¬¡', fontsize=12)
    
    ax2.set_title('å­¦ä¹ ç‡å˜åŒ–æ›²çº¿', fontweight='bold', fontsize=14)
    ax2.set_ylabel('å­¦ä¹ ç‡', fontsize=12)
    ax2.grid(True, linestyle='--', alpha=0.7)
    ax2.yaxis.set_major_formatter(plt.FormatStrFormatter('%.6f'))
    ax2.legend(loc='upper right')
    
    # 3. æ¯è½®è®­ç»ƒæ—¶é—´
    ax3.plot(epochs, stats['epoch_times'], 'o-', linewidth=2, markersize=8, 
             color=colors['time'], label='è®­ç»ƒæ—¶é—´')
    
    # è®¡ç®—å¹³å‡è®­ç»ƒæ—¶é—´
    avg_time = sum(stats['epoch_times']) / len(stats['epoch_times'])
    ax3.axhline(y=avg_time, linestyle='--', color='gray', alpha=0.8)
    ax3.annotate(f'å¹³å‡: {avg_time:.2f}ç§’',
                xy=(len(epochs) / 2, avg_time),
                xytext=(len(epochs) / 2, avg_time * 1.1),
                fontsize=11)
    
    ax3.set_title('æ¯è½®è®­ç»ƒæ—¶é—´', fontweight='bold', fontsize=14)
    ax3.set_xlabel('è½®æ¬¡', fontsize=12)
    ax3.set_ylabel('æ—¶é—´(ç§’)', fontsize=12)
    ax3.grid(True, linestyle='--', alpha=0.7)
    ax3.legend(loc='upper right')
    
    # 4. GPUå†…å­˜ä½¿ç”¨
    if 'gpu_memory_usage' in stats and stats['gpu_memory_usage']:
        memory_usage_gb = [m/1024**3 for m in stats['gpu_memory_usage']]
        ax4.plot(epochs, memory_usage_gb, 'o-', linewidth=2, markersize=8, 
                 color=colors['memory'], label='GPUå†…å­˜')
        
        # æ·»åŠ æœ€å¤§å†…å­˜ä½¿ç”¨æ ‡è®°
        max_mem = max(memory_usage_gb)
        max_idx = memory_usage_gb.index(max_mem)
        ax4.annotate(f'æœ€å¤§å€¼: {max_mem:.2f}GB',
                    xy=(max_idx + 1, max_mem), 
                    xytext=(max_idx + 1 - 0.5, max_mem * 1.1),
                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),
                    fontsize=11)
        
        ax4.set_title('GPUå†…å­˜ä½¿ç”¨', fontweight='bold', fontsize=14)
        ax4.set_xlabel('è½®æ¬¡', fontsize=12)
        ax4.set_ylabel('å†…å­˜ä½¿ç”¨(GB)', fontsize=12)
        ax4.grid(True, linestyle='--', alpha=0.7)
        ax4.legend(loc='upper right')
    else:
        # å¦‚æœæ²¡æœ‰GPUä¿¡æ¯ï¼Œåˆ™æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
        ax4.axis('off')
        ax4.text(0.5, 0.5, 'è®­ç»ƒæ€»ç»“:\n\n' + 
                f'æ€»è®­ç»ƒè½®æ•°: {len(epochs)}\n' +
                f'æœ€ç»ˆæŸå¤±: {stats["losses"][-1]:.4f}\n' +
                f'æœ€å°æŸå¤±: {min(stats["losses"]):.4f}\n' +
                f'æ€»è®­ç»ƒæ—¶é—´: {sum(stats["epoch_times"]):.2f}ç§’',
                horizontalalignment='center',
                verticalalignment='center',
                fontsize=14,
                transform=ax4.transAxes)
    
    # æ·»åŠ æ•´ä½“æ ‡é¢˜
    fig.suptitle('çµçŒ«å¢¨éŸµ - è®­ç»ƒç»Ÿè®¡å›¾è¡¨', fontsize=18, fontweight='bold', y=0.98)
    
    # æ·»åŠ è®­ç»ƒæ—¶é—´æˆ³å’Œç‰ˆæœ¬ä¿¡æ¯
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    fig.text(0.5, 0.01, f'è®­ç»ƒæ—¶é—´: {timestamp} | ç‰ˆæœ¬: v{VERSION}',
             horizontalalignment='center', fontsize=10, alpha=0.7)
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # ä¿å­˜å›¾è¡¨ä¸ºé«˜è´¨é‡PNGæ–‡ä»¶
    stats_plot_path = os.path.join(save_dir, f'training_stats_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
    plt.savefig(stats_plot_path, dpi=300, bbox_inches='tight')
    
    # åŒæ—¶ä¿å­˜ä¸ºPDFä»¥ä¾¿è¿›ä¸€æ­¥ç¼–è¾‘æˆ–å‘å¸ƒ
    pdf_path = os.path.join(save_dir, f'training_stats_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pdf')
    plt.savefig(pdf_path, format='pdf', bbox_inches='tight')
    
    plt.close()
    
    log_success(f"è®­ç»ƒç»Ÿè®¡å›¾è¡¨å·²ä¿å­˜è‡³: {stats_plot_path}")
    log_info(f"PDFç‰ˆæœ¬å·²ä¿å­˜è‡³: {pdf_path}")

# ================= æ—¥å¿—ç³»ç»Ÿ =================

def setup_logger():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = os.path.join("logs", "train_model")
    os.makedirs(log_dir, exist_ok=True)
    
    # å…ˆè·å–æ ¹æ—¥å¿—è®°å½•å™¨å¹¶ç§»é™¤æ‰€æœ‰å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤æ—¥å¿—
    root_logger = logging.getLogger()
    if root_logger.handlers:
        for handler in root_logger.handlers:
            root_logger.removeHandler(handler)
    
    logger = logging.getLogger("LingmaoMoyun")
    logger.setLevel(logging.INFO)
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤æ·»åŠ 
    if logger.handlers:
        logger.handlers.clear()
    
    # æ§åˆ¶å°å¤„ç†å™¨ - ä½¿ç”¨ç®€åŒ–æ ¼å¼ï¼Œä¸æ˜¾ç¤ºæ—¥æœŸæ—¶é—´
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨ - ä¿ç•™å®Œæ•´æ—¥å¿—æ ¼å¼
    log_file = os.path.join(log_dir, f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    
    # è®¾ç½®ç®€æ´æ ¼å¼ï¼ˆæ§åˆ¶å°ä¸æ˜¾ç¤ºæ—¶é—´ï¼‰å’Œå®Œæ•´æ ¼å¼ï¼ˆæ–‡ä»¶ä¿ç•™æ—¶é—´ï¼‰
    console_formatter = logging.Formatter("%(message)s")
    file_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    
    console_handler.setFormatter(console_formatter)
    file_handler.setFormatter(file_formatter)
    
    # æ·»åŠ å¤„ç†å™¨
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

logger = setup_logger()

def log_info(message):
    """è¾“å‡ºä¿¡æ¯æ—¥å¿—"""
    logger.info(message)

def log_warning(message):
    """è¾“å‡ºè­¦å‘Šæ—¥å¿—"""
    logger.warning(message)

def log_error(message):
    """è¾“å‡ºé”™è¯¯æ—¥å¿—"""
    logger.error(message)

def log_success(message):
    """è¾“å‡ºæˆåŠŸæ—¥å¿—"""
    logger.info(message)

# ç§»é™¤åˆ†éš”çº¿å‡½æ•°ï¼Œç®€åŒ–ä¸ºæ™®é€šæ ‡é¢˜è¾“å‡º
def print_section_header(title):
    """æ‰“å°æ®µè½æ ‡é¢˜ï¼Œä¸å¸¦åˆ†éš”ç¬¦"""
    logger.info(f"\n{title}")

# ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºéšæ—¶ç»ˆæ­¢è®­ç»ƒå¹¶ä¿å­˜
terminate_training = False
def signal_handler(sig, frame):
    global terminate_training
    log_warning("æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å½“å‰çŠ¶æ€...")
    terminate_training = True
    
    # è®¾ç½®ä¸€ä¸ªå®šæ—¶å™¨ï¼Œå¦‚æœ10ç§’å†…æœªèƒ½æ­£å¸¸é€€å‡ºï¼Œåˆ™å¼ºåˆ¶é€€å‡º
    import threading
    def force_exit():
        log_error("ä¿å­˜è¶…æ—¶ï¼Œå¼ºåˆ¶é€€å‡ºç¨‹åº")
        import os
        os._exit(1)
    
    # 10ç§’åå¼ºåˆ¶é€€å‡º
    timer = threading.Timer(10.0, force_exit)
    timer.start()

signal.signal(signal.SIGINT, signal_handler)

# ================= æ•°æ®å¤„ç†éƒ¨åˆ† =================

class LMDataset(Dataset):
    """è¯­è¨€æ¨¡å‹æ•°æ®é›†ï¼Œç”¨äºè‡ªå›å½’è®­ç»ƒ"""
    def __init__(self, data_path, context_length=512, tokenizer=None, stride=256, max_chunks=None):
        """
        åˆå§‹åŒ–è¯­è¨€æ¨¡å‹æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®è·¯å¾„ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•
            context_length: ä¸Šä¸‹æ–‡é•¿åº¦
            tokenizer: åˆ†è¯å™¨å®ä¾‹
            stride: æ»‘åŠ¨çª—å£æ­¥é•¿
            max_chunks: æœ€å¤§æ–‡æœ¬å—æ•°é‡ï¼Œç”¨äºé™åˆ¶å†…å­˜ä½¿ç”¨
        """
        self.context_length = context_length
        self.tokenizer = tokenizer
        self.stride = stride
        self.max_chunks = max_chunks
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨numpyæ•°ç»„è€ŒéPythonåˆ—è¡¨
        import numpy as np
        self.np = np
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶å­˜å‚¨å¤§å‹æ•°æ®
        self.use_memmap = False
        self.memmap_path = None
        
        # åŠ è½½æ‰€æœ‰æ–‡æœ¬
        self.token_chunks = []
        self.samples = []
        
        # è®¡æ—¶å™¨
        self.load_time = 0
        self.process_time = 0
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        self.peak_memory_mb = 0
        self._track_memory()
        
        # åŠ è½½æ•°æ®
        start_time = time.time()
        self._load_data(data_path)
        self.load_time = time.time() - start_time
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
        start_time = time.time()
        self._create_samples()
        self.process_time = time.time() - start_time
        
        self._track_memory()
        logger.info(f"æ•°æ®é›† {data_path} åŠ è½½å®Œæˆï¼Œå…±æœ‰ {len(self.samples)} ä¸ªæ ·æœ¬ï¼Œå³°å€¼å†…å­˜: {self.peak_memory_mb:.2f}MB")
    
    def _track_memory(self):
        """è·Ÿè¸ªå†…å­˜ä½¿ç”¨"""
        import psutil
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        current_memory_mb = memory_info.rss / (1024 * 1024)
        self.peak_memory_mb = max(self.peak_memory_mb, current_memory_mb)
    
    def _load_data(self, data_path):
        """
        ä»æ–‡ä»¶æˆ–ç›®å½•åŠ è½½æ•°æ®
        
        Args:
            data_path: æ•°æ®è·¯å¾„ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•
        """
        data_path = Path(data_path)
        if not data_path.exists():
            raise FileNotFoundError(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨è¿­ä»£å™¨è€Œéåˆ—è¡¨
        if data_path.is_dir():
            files_to_process = data_path.glob("**/*.jsonl")
            logger.info(f"ä»ç›®å½• {data_path} åŠ è½½JSONLæ–‡ä»¶")
        else:
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥æ·»åŠ 
            files_to_process = [data_path]
            logger.info(f"åŠ è½½å•ä¸ªæ–‡ä»¶: {data_path}")
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        chunks_processed = 0
        for file_path in tqdm(files_to_process, 
                            desc="åŠ è½½æ•°æ®æ–‡ä»¶", 
                            ncols=100, 
                            colour="green",
                            leave=True,
                            smoothing=0.1):
            try:
                new_chunks = self._process_jsonl_file(file_path)
                chunks_processed += new_chunks
                logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}, æå–äº† {new_chunks} ä¸ªæ–‡æœ¬å—")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å—æ•°é™åˆ¶
                if self.max_chunks and chunks_processed >= self.max_chunks:
                    logger.info(f"å·²è¾¾åˆ°æœ€å¤§å—æ•°é™åˆ¶ ({self.max_chunks})ï¼Œåœæ­¢åŠ è½½")
                    break
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        
        logger.info(f"å…±åŠ è½½äº† {len(self.token_chunks)} ä¸ªæ–‡æœ¬å—")
    
    def _process_jsonl_file(self, file_path):
        """
        å¤„ç†å•ä¸ªJSONLæ–‡ä»¶ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
        
        Args:
            file_path: JSONLæ–‡ä»¶è·¯å¾„
        
        Returns:
            å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
        """
        new_chunks = 0
        # ä¼˜åŒ–ï¼šä½¿ç”¨è¿­ä»£å™¨è¯»å–æ–‡ä»¶è€Œéä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è¡Œ
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(tqdm(f, 
                                              desc=f"å¤„ç† {Path(file_path).name}", 
                                              leave=False, 
                                              ncols=100, 
                                              colour="blue",
                                              dynamic_ncols=True)):
                try:
                    # è§£æJSONè¡Œ
                    line = line.strip()
                    if not line:
                        continue
                    
                    item = json.loads(line)
                    
                    # æå–æ–‡æœ¬å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨contentå­—æ®µ
                    content = None
                    
                    # å°è¯•ä¸åŒçš„å­—æ®µåç§°
                    for field in ['content', 'text', 'body', 'paragraphs']:
                        if field in item and item[field]:
                            content = item[field]
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
                    if content is None and 'title' in item:
                        content = item.get('title', '')
                    
                    # å¦‚æœå†…å®¹æ˜¯åˆ—è¡¨ï¼Œåˆå¹¶ä¸ºå­—ç¬¦ä¸²
                    if isinstance(content, list):
                        content = '\n'.join([str(p) for p in content if p])
                    
                    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
                    if not isinstance(content, str) or not content.strip():
                        continue
                    
                    # æ·»åŠ åˆ°tokenå—
                    self.token_chunks.append(content)
                    new_chunks += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å—æ•°é™åˆ¶
                    if self.max_chunks and len(self.token_chunks) >= self.max_chunks:
                        break
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†ç¬¬ {line_idx+1} è¡Œæ—¶å‡ºé”™: {str(e)}")
                    
        return new_chunks
    
    def _create_samples(self):
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬ï¼Œä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜æ€§èƒ½"""
        logger.info("å¼€å§‹åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
        start_time = time.time()
        
        # å¦‚æœæœ‰åˆ†è¯å™¨ï¼Œä½¿ç”¨åˆ†è¯å™¨å¤„ç†æ–‡æœ¬
        all_tokens = []
        
        # æ‰¹é‡å¤„ç†æ–‡æœ¬
        batch_size = 100  # æ‰¹å¤„ç†å¤§å°
        for i in range(0, len(self.token_chunks), batch_size):
            batch = self.token_chunks[i:i+batch_size]
            
            if self.tokenizer:
                # ä½¿ç”¨æ‰¹é‡åˆ†è¯ä¼˜åŒ–
                if hasattr(self.tokenizer, 'batch_tokenize'):
                    token_batches = self.tokenizer.batch_tokenize(batch)
                    for tokens in token_batches:
                        if tokens and len(tokens) > 1:
                            all_tokens.extend(tokens)
                else:
                    # ä¼ ç»Ÿæ–¹å¼
                    for chunk in batch:
                        try:
                            tokens = self.tokenizer.tokenize(chunk)
                            if tokens and len(tokens) > 1:
                                all_tokens.extend(tokens)
                        except Exception as e:
                            logger.warning(f"åˆ†è¯æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            else:
                # å­—ç¬¦çº§ç¼–ç 
                for chunk in batch:
                    chars = list(chunk)
                    # å°†å­—ç¬¦è½¬æ¢ä¸ºç®€å•çš„æ•´æ•°ID
                    char_ids = [ord(c) % 30000 for c in chars]
                    if char_ids:
                        all_tokens.extend(char_ids)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„token
        if len(all_tokens) < self.context_length:
            logger.warning(f"æ€»tokenæ•° {len(all_tokens)} å°äºä¸Šä¸‹æ–‡é•¿åº¦ {self.context_length}ï¼Œå°†é‡å¤æ•°æ®")
            # é‡å¤æ•°æ®ç›´åˆ°è¾¾åˆ°è¦æ±‚
            repeats = (self.context_length // len(all_tokens)) + 1
            all_tokens = all_tokens * repeats
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨numpyæ•°ç»„
        all_tokens_array = self.np.array(all_tokens, dtype=self.np.int32)
        
        # ä¼˜åŒ–ï¼šå¯¹äºå¤§å‹æ•°æ®é›†ï¼Œä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶
        if len(all_tokens_array) > 10_000_000:  # 1åƒä¸‡ä¸ªtokenä»¥ä¸Šä½¿ç”¨å†…å­˜æ˜ å°„
            self.use_memmap = True
            import tempfile
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
            fd, self.memmap_path = tempfile.mkstemp(suffix='.dat')
            os.close(fd)  # å…³é—­æ–‡ä»¶æè¿°ç¬¦
            
            # åˆ›å»ºå†…å­˜æ˜ å°„æ–‡ä»¶
            tokens_memmap = self.np.memmap(self.memmap_path, 
                                      dtype=self.np.int32, 
                                      mode='w+', 
                                      shape=all_tokens_array.shape)
            
            # å†™å…¥æ•°æ®
            tokens_memmap[:] = all_tokens_array[:]
            tokens_memmap.flush()
            
            # é‡æ–°æ‰“å¼€ä»¥åªè¯»æ¨¡å¼
            all_tokens_array = self.np.memmap(self.memmap_path, 
                                         dtype=self.np.int32, 
                                         mode='r', 
                                         shape=all_tokens_array.shape)
            
            logger.info(f"ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶å­˜å‚¨ {len(all_tokens_array)} ä¸ªtoken")
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬ - å‘é‡åŒ–å¤„ç†
        total_samples = max(0, (len(all_tokens_array) - self.context_length) // self.stride + 1)
        logger.info(f"å°†åˆ›å»º {total_samples} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        if total_samples > 0:
            # é¢„åˆ†é…å†…å­˜
            input_arrays = []
            target_arrays = []
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=total_samples, 
                     desc="åˆ›å»ºè®­ç»ƒæ ·æœ¬", 
                     ncols=100, 
                     colour="cyan",
                     leave=True) as pbar:
                
                for i in range(0, len(all_tokens_array) - self.context_length, self.stride):
                    # åˆ‡ç‰‡è·å–è¾“å…¥å’Œç›®æ ‡
                    input_slice = all_tokens_array[i:i + self.context_length]
                    target_slice = all_tokens_array[i + 1:i + self.context_length + 1]
                    
                    if len(input_slice) == self.context_length and len(target_slice) == self.context_length:
                        input_arrays.append(input_slice)
                        target_arrays.append(target_slice)
                    
                    pbar.update(1)
                    
                    # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨
                    if len(input_arrays) % 1000 == 0:
                        self._track_memory()
            
            # å°†æ‰€æœ‰æ ·æœ¬åˆå¹¶ä¸ºä¸€ä¸ªåˆ—è¡¨
            for i in range(len(input_arrays)):
                self.samples.append({
                    'input_ids': input_arrays[i],
                    'target_ids': target_arrays[i]
                })
        
        elapsed = time.time() - start_time
        logger.info(f"æ ·æœ¬åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’ï¼Œå…±åˆ›å»º {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # è·å–æ ·æœ¬
        sample = self.samples[idx]
        
        # è½¬æ¢ä¸ºtensorä»¥ä¾¿è®­ç»ƒ
        input_tensor = torch.tensor(sample['input_ids'], dtype=torch.long)
        target_tensor = torch.tensor(sample['target_ids'], dtype=torch.long)
        
        return input_tensor, target_tensor
        
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        # å¦‚æœä½¿ç”¨äº†å†…å­˜æ˜ å°„æ–‡ä»¶ï¼Œåœ¨ææ„æ—¶åˆ é™¤
        if self.use_memmap and self.memmap_path and os.path.exists(self.memmap_path):
            try:
                os.unlink(self.memmap_path)
                logger.debug(f"å·²åˆ é™¤ä¸´æ—¶å†…å­˜æ˜ å°„æ–‡ä»¶: {self.memmap_path}")
            except:
                pass

# ================= æ¨¡å‹å®šä¹‰ =================

class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=12, dim_feedforward=3072, dropout=0.1, max_len=1024):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_len)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        self.output_layer = nn.Linear(d_model, vocab_size)
        self.d_model = d_model
        self.max_len = max_len
        self._init_parameters()
    
    def _init_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
                
    def forward(self, src):
        # src shape: [batch_size, seq_len]
        
        # å°†è¾“å…¥è½¬æ¢ä¸ºåµŒå…¥å‘é‡
        src = self.embedding(src) * math.sqrt(self.d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        src = self.pos_encoder(src)
        
        # é€šè¿‡Transformerç¼–ç å™¨
        output = self.transformer_encoder(src)
        
        # ç”Ÿæˆé¢„æµ‹
        output = self.output_layer(output)
        
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

# ================= è®­ç»ƒéƒ¨åˆ† =================

# æ·»åŠ å¤œé—´æ¨¡å¼æ£€æµ‹
def is_night_mode():
    """æ£€æŸ¥å½“å‰æ˜¯å¦åº”è¯¥ä½¿ç”¨å¤œé—´æ¨¡å¼ï¼ˆæ™šä¸Š9ç‚¹åˆ°æ—©ä¸Š8ç‚¹ï¼‰"""
    current_hour = datetime.now().hour
    return current_hour >= 21 or current_hour < 8

# æ·»åŠ è®¾ç½®CPUé™åˆ¶çš„å‡½æ•°
def set_cpu_limit(limit_cores=None):
    """é™åˆ¶CPUæ ¸å¿ƒä½¿ç”¨"""
    if limit_cores is None:
        # è‡ªåŠ¨è®¾ç½®ï¼Œå¤œé—´æ¨¡å¼ä½¿ç”¨è¾ƒå°‘æ ¸å¿ƒ
        if is_night_mode():
            limit_cores = max(2, psutil.cpu_count(logical=False) // 2)
        else:
            limit_cores = psutil.cpu_count(logical=False)
    
    # å°è¯•è®¾ç½®è¿›ç¨‹äº²å’Œæ€§
    try:
        p = psutil.Process()
        if limit_cores < psutil.cpu_count():
            # è®¾ç½®CPUäº²å’Œæ€§ï¼Œä½¿ç”¨å‰Nä¸ªCPUæ ¸å¿ƒ
            p.cpu_affinity(list(range(limit_cores)))
        return True
    except Exception as e:
        log_warning(f"æ— æ³•è®¾ç½®CPUé™åˆ¶: {str(e)}")
        return False

# æ·»åŠ GPUå†…å­˜é™åˆ¶å‡½æ•°
def limit_gpu_memory(percent=None):
    """é™åˆ¶GPUå†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”"""
    if not torch.cuda.is_available():
        return False
    
    if percent is None:
        # è‡ªåŠ¨è®¾ç½®ï¼Œå¤œé—´æ¨¡å¼ä½¿ç”¨è¾ƒå°‘å†…å­˜
        percent = 50 if is_night_mode() else 90
    
    try:
        # è®¾ç½®å†…å­˜é™åˆ¶
        total_memory = torch.cuda.get_device_properties(0).total_memory
        limit = int(total_memory * percent / 100)
        torch.cuda.set_per_process_memory_fraction(percent / 100)
        return True
    except Exception as e:
        log_warning(f"æ— æ³•è®¾ç½®GPUå†…å­˜é™åˆ¶: {str(e)}")
        return False

# æ·»åŠ æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹çš„å‡½æ•°
def find_latest_checkpoint(model_save_dir):
    """æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_files = glob.glob(os.path.join(model_save_dir, f"checkpoint_epoch*_v{VERSION}.pt"))
    if not checkpoint_files:
        return None
    
    # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åº
    checkpoint_files.sort(key=os.path.getmtime, reverse=True)
    return checkpoint_files[0]

def save_checkpoint(epoch, model, optimizer, scheduler, scaler, stats, save_dir, name=None, is_best=False):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        scaler: æ¢¯åº¦ç¼©æ”¾å™¨(AMP)
        stats: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        save_dir: ä¿å­˜ç›®å½•
        name: æ£€æŸ¥ç‚¹åç§°
        is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True, parents=True)
    
    # é»˜è®¤åç§°
    if name is None:
        name = f"checkpoint_epoch_{epoch+1}"
    
    # æ„å»ºä¿å­˜è·¯å¾„
    checkpoint_path = save_dir / f"{name}.pt"
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'scaler': scaler.state_dict() if scaler else None,
        'stats': stats
    }
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜è‡³: {checkpoint_path}")
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
    if is_best:
        best_path = save_dir / "best_model.pt"
        torch.save(checkpoint, best_path)
        logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜è‡³: {best_path}")
        
    return checkpoint_path

def train_model(
    train_file, 
    test_file=None,
    model_save_dir="model_weights",
    tokenizer_path="tokenizer.json",
    context_length=512,
    batch_size=8,
    learning_rate=5e-5,
    epochs=10,
    d_model=768,
    nhead=12,
    num_layers=12,
    dim_feedforward=3072,
    dropout=0.1,
    use_amp=True,
    checkpoint_every=1,
    accumulation_steps=4,
    device=None,
    max_grad_norm=1.0,
    weight_decay=0.01,
    resume_from=None,
    auto_resume=True,
    night_mode=True,
    save_on_interrupt=True
):
    """
    è®­ç»ƒè¯­è¨€æ¨¡å‹
    
    Args:
        train_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºéªŒè¯
        model_save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        tokenizer_path: åˆ†è¯å™¨è·¯å¾„
        context_length: ä¸Šä¸‹æ–‡é•¿åº¦
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        epochs: è®­ç»ƒè½®æ•°
        d_model: æ¨¡å‹ç»´åº¦
        nhead: æ³¨æ„åŠ›å¤´æ•°
        num_layers: Transformerå±‚æ•°
        dim_feedforward: å‰é¦ˆç½‘ç»œç»´åº¦
        dropout: Dropoutç‡
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        checkpoint_every: æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        device: è®­ç»ƒè®¾å¤‡
        max_grad_norm: æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°
        weight_decay: æƒé‡è¡°å‡ç³»æ•°
        resume_from: ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        auto_resume: è‡ªåŠ¨æ¢å¤æœ€è¿‘çš„æ£€æŸ¥ç‚¹
        night_mode: æ˜¯å¦å¯ç”¨å¤œé—´ä½åŠŸè€—æ¨¡å¼
        save_on_interrupt: ä¸­æ–­æ—¶æ˜¯å¦ä¿å­˜
    """
    # é¿å…å±€éƒ¨å˜é‡ä¸å…¨å±€å˜é‡å†²çªï¼Œé‡æ–°å¯¼å…¥å¿…è¦æ¨¡å—
    import os as os_local
    from pathlib import Path as Path_local
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print_section_header("çµçŒ«å¢¨éŸµ è¯­è¨€æ¨¡å‹è®­ç»ƒ")
    logger.info(f"è®­ç»ƒç‰ˆæœ¬: v{VERSION}")
    logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯ï¼Œæ›´ç®€æ´çš„æ ¼å¼
    print_section_header("è®­ç»ƒé…ç½®")
    
    # ä½¿ç”¨ç®€æ´æ ¼å¼æ‰“å°æ ¸å¿ƒé…ç½®
    config_info = [
        f"è®­ç»ƒæ•°æ®: {train_file}",
        f"æµ‹è¯•æ•°æ®: {'æ— ' if not test_file else test_file}",
        f"æ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir}",
        f"ä¸Šä¸‹æ–‡é•¿åº¦: {context_length}",
        f"æ‰¹æ¬¡å¤§å°: {batch_size} (æœ‰æ•ˆæ‰¹é‡å¤§å°: {batch_size * accumulation_steps})",
        f"å­¦ä¹ ç‡: {learning_rate}",
        f"è®­ç»ƒè½®æ•°: {epochs}"
    ]
    
    # ç®€æ´æ‰“å°æ¨¡å‹æ¶æ„å‚æ•°
    model_info = [
        f"æ¨¡å‹ç»´åº¦: {d_model}",
        f"æ³¨æ„åŠ›å¤´æ•°: {nhead}",
        f"Transformerå±‚æ•°: {num_layers}",
        f"å‰é¦ˆç½‘ç»œç»´åº¦: {dim_feedforward}",
        f"Dropoutç‡: {dropout}"
    ]
    
    # ç®€æ´æ‰“å°è®­ç»ƒç­–ç•¥å‚æ•°
    train_info = [
        f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {accumulation_steps}",
        f"æ¢¯åº¦è£å‰ªé˜ˆå€¼: {max_grad_norm}",
        f"æƒé‡è¡°å‡: {weight_decay}",
        f"æ··åˆç²¾åº¦è®­ç»ƒ: {'æ˜¯' if use_amp else 'å¦'}",
        f"å¤œé—´æ¨¡å¼: {'æ˜¯' if night_mode else 'å¦'}",
        f"è‡ªåŠ¨æ¢å¤è®­ç»ƒ: {'æ˜¯' if auto_resume else 'å¦'}"
    ]
    
    # åˆ†ç»„æ‰“å°å‚æ•°
    for info in config_info:
        logger.info(info)
    logger.info("æ¨¡å‹ç»“æ„:")
    for info in model_info:
        logger.info(f"  {info}")
    logger.info("è®­ç»ƒç­–ç•¥:")
    for info in train_info:
        logger.info(f"  {info}")
    
    # åº”ç”¨å¤œé—´æ¨¡å¼è®¾ç½®
    if night_mode and is_night_mode():
        logger.info("ğŸŒ™ æ£€æµ‹åˆ°å¤œé—´æ—¶æ®µï¼Œå¯ç”¨ä½åŠŸè€—æ¨¡å¼")
        set_cpu_limit()
        # å¤œé—´æ¨¡å¼ä¸‹è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯
        original_batch_size = batch_size
        batch_size = max(2, batch_size // 2)
        accumulation_steps = accumulation_steps * original_batch_size // batch_size
        logger.info(f"å¤œé—´æ¨¡å¼: æ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º {batch_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°è°ƒæ•´ä¸º {accumulation_steps}")
    
    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    os_local.makedirs(model_save_dir, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    if device is None:
        if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("è®­ç»ƒè®¾å¤‡: mps (Apple Silicon GPU)")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info(f"è®­ç»ƒè®¾å¤‡: cuda ({torch.cuda.get_device_name(0)})")
            # åœ¨å¤œé—´æ¨¡å¼ä¸‹é™åˆ¶GPUå†…å­˜ä½¿ç”¨
            if night_mode and is_night_mode():
                limit_gpu_memory(50)  # å¤œé—´ä½¿ç”¨50%GPUå†…å­˜
        else:
            device = torch.device("cpu")
            logger.info("è®­ç»ƒè®¾å¤‡: cpu")
    
    device_info = f"è®­ç»ƒè®¾å¤‡: {device}"
    if device.type == 'cuda':
        device_info += f" ({torch.cuda.get_device_name(0)})"
        log_info(f"GPUå†…å­˜: æ€»è®¡ {format_memory_size(torch.cuda.get_device_properties(0).total_memory)}")
    elif device.type == 'mps':
        device_info += " (Apple Silicon GPU)"
    log_info(device_info)
    
    # åŠ è½½åˆ†è¯å™¨
    print_section_header("åŠ è½½åˆ†è¯å™¨")
    vocab_size = 50000  # é»˜è®¤è¯æ±‡è¡¨å¤§å°
    
    if os_local.path.exists(tokenizer_path):
        try:
            from tokenizer import ClassicalTokenizer
            tokenizer = ClassicalTokenizer()
            tokenizer.load(tokenizer_path)
            logger.info("åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡é‡ï¼š{}".format(len(tokenizer.token_to_id)))
            vocab_size = len(tokenizer.token_to_id)
        except Exception as e:
            error_msg = f"åŠ è½½åˆ†è¯å™¨æ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ç¬¦çº§åˆ†è¯"
            logger.warning(error_msg)
            tokenizer = None
    else:
        logger.warning(f"åˆ†è¯å™¨æ–‡ä»¶ {tokenizer_path} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ç¬¦çº§åˆ†è¯")
        tokenizer = None
    
    # åŠ è½½æ•°æ®é›†
    print_section_header("åŠ è½½æ•°æ®é›†")
    logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {train_file}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LMDataset(train_file, context_length, tokenizer, stride=context_length//2)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # ä½¿ç”¨0ä»¥é¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory=True if device != "cpu" else False,
    )
    
    # å¯é€‰çš„æµ‹è¯•æ•°æ®é›†
    test_loader = None
    if test_file and os_local.path.exists(test_file):
        logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
        test_dataset = LMDataset(
            test_file, context_length, tokenizer, stride=context_length
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if device != "cpu" else False,
        )
    
    # åˆ›å»ºæ¨¡å‹
    print_section_header("åˆ›å»ºæ¨¡å‹")
    logger.info("åˆå§‹åŒ–æ¨¡å‹æ¶æ„...")
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTransformer(
        vocab_size=vocab_size,
        d_model=d_model,
        nhead=nhead,
        num_layers=num_layers,
        dim_feedforward=dim_feedforward,
        dropout=dropout
    ).to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    param_count = sum(p.numel() for p in model.parameters())
    logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
    
    # æ£€æŸ¥æ˜¯å¦ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤
    if resume_from:
        if os_local.path.exists(resume_from):
            logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from}")
            checkpoint = torch.load(resume_from, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            initial_epoch = checkpoint.get('epoch', 0)
            best_loss = checkpoint.get('best_loss', float('inf'))
            logger.info(f"ä»ç¬¬ {initial_epoch + 1} è½®ç»§ç»­è®­ç»ƒ")
        else:
            logger.warning(f"æŒ‡å®šçš„æ£€æŸ¥ç‚¹ {resume_from} ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            initial_epoch = 0
            best_loss = float('inf')
    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤
    elif auto_resume:
        latest_checkpoint = find_latest_checkpoint(model_save_dir)
        if latest_checkpoint:
            logger.info(f"æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")
            try:
                checkpoint = torch.load(latest_checkpoint, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                initial_epoch = checkpoint.get('epoch', 0)
                best_loss = checkpoint.get('best_loss', float('inf'))
                logger.info(f"ä»ç¬¬ {initial_epoch + 1} è½®ç»§ç»­è®­ç»ƒ")
            except Exception as e:
                logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å‡ºé”™: {str(e)}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                initial_epoch = 0
                best_loss = float('inf')
        else:
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆæ£€æŸ¥ç‚¹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            initial_epoch = 0
            best_loss = float('inf')
    else:
        initial_epoch = 0
        best_loss = float('inf')
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    logger.info("è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    # å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå°è¯•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if 'optimizer_state_dict' in locals().get('checkpoint', {}):
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info("å·²æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€")
        except:
            logger.warning("æ— æ³•æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨")
    
    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    def lr_lambda(current_step):
        # çº¿æ€§é¢„çƒ­åçš„ä½™å¼¦é€€ç«
        warmup_steps = 1000
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, 10000 - warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = None
    if use_amp:
        # ä»…åœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        if device.type == 'cuda':
            logger.info("æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ï¼Œå°†æå‡è®­ç»ƒé€Ÿåº¦")
            scaler = GradScaler()
        else:
            logger.info("å½“å‰è®¾å¤‡ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
            use_amp = False
    
    # è®­ç»ƒç»Ÿè®¡
    stats = {
        'losses': [],
        'learning_rates': [],
        'epoch_times': [],
        'gpu_memory_usage': [] if torch.cuda.is_available() else None
    }
    
    # è®°å½•åˆå§‹å­¦ä¹ ç‡
    stats['learning_rates'].append(scheduler.get_last_lr()[0])
    
    # å¼€å§‹è®­ç»ƒ
    print_section_header("å¼€å§‹è®­ç»ƒ")
    logger.info(f"å…± {epochs} è½®è®­ç»ƒï¼Œä»ç¬¬ {initial_epoch+1} è½®å¼€å§‹")
    
    # è®¾ç½®æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(initial_epoch, epochs):
        # æ‰“å°å½“å‰è½®æ¬¡ä¿¡æ¯
        print_section_header(f"ç¬¬ {epoch+1}/{epochs} è½®è®­ç»ƒ")
        
        # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        model.train()
        
        # è®­ç»ƒå¾ªç¯å˜é‡
        total_loss = 0.0
        num_batches = len(train_loader)
        batch_times = []
        
        # ä½¿ç”¨ç®€åŒ–çš„è¿›åº¦æ¡
        progress_bar = tqdm(train_loader, 
                          desc=f"ç¬¬ {epoch+1}/{epochs} è½®è®­ç»ƒ",
                          ncols=100, 
                          leave=True,
                          dynamic_ncols=True,
                          position=0,
                          ascii=False,
                          smoothing=0.1)
        
        # é‡ç½®ä¼˜åŒ–å™¨æ¢¯åº¦
        optimizer.zero_grad()
        
        for step, batch in enumerate(progress_bar):
            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            try:
                # å¤„ç†æ–°æ—§ä¸¤ç§æ•°æ®æ ¼å¼ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                if isinstance(batch, dict):
                    # æ—§æ ¼å¼ï¼šå­—å…¸å½¢å¼{'input_ids': tensor, 'target_ids': tensor}
                    input_ids = batch['input_ids'].to(device)
                    target_ids = batch['target_ids'].to(device)
                elif isinstance(batch, tuple) and len(batch) == 2:
                    # æ–°æ ¼å¼ï¼šå…ƒç»„å½¢å¼(input_tensor, target_tensor)
                    input_ids, target_ids = batch
                    input_ids = input_ids.to(device)
                    target_ids = target_ids.to(device)
                else:
                    logger.warning(f"æœªçŸ¥çš„æ‰¹å¤„ç†æ ¼å¼: {type(batch)}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast(enabled=use_amp):
                    # å‰å‘è®¡ç®—
                    outputs = model(input_ids)  # [B, L, V]
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))
                    loss = loss / accumulation_steps  # æ¢¯åº¦ç´¯ç§¯
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
                if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):
                    # æ¢¯åº¦è£å‰ª
                    if max_grad_norm > 0:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                    
                    # æ›´æ–°å‚æ•°
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()
                    
                    # é‡ç½®ç´¯ç§¯è®¡æ•°å™¨
                    steps_since_update = 0
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                total_loss += loss.item() * accumulation_steps
                current_lr = scheduler.get_last_lr()[0]
                average_loss = total_loss / (step + 1)
                
                progress_bar.set_postfix({
                    'loss': f"{average_loss:.4f}",
                    'lr': f"{current_lr:.8f}"
                })
                
                # æ›´æ–°è®­ç»ƒç»Ÿè®¡
                current_step = epoch * len(train_loader) + step
                
                # è®°å½•è®­ç»ƒç»Ÿè®¡
                stats['losses'].append(average_loss)
                stats['learning_rates'].append(current_lr)
                stats['steps'].append(current_step)
                
                # è®°å½•GPUå†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    stats['gpu_memory_usage'].append(torch.cuda.memory_allocated(0))
                
            except Exception as e:
                logger.error(f"è®­ç»ƒä¸­å‡ºé”™ (step {step}): {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                continue
        
        # è®¡ç®—epochå¹³å‡æŸå¤±
        epoch_loss = total_loss / len(train_loader)
        epoch_end_time = time.time()
        epoch_time = epoch_end_time - epoch_start_time
        stats['epoch_times'].append(epoch_time)
        
        # è¾“å‡ºepochç»Ÿè®¡ä¿¡æ¯
        logger.info(f"Epoch {epoch+1}/{epochs} å®Œæˆ: æŸå¤± = {epoch_loss:.4f}, è€—æ—¶ = {format_time(epoch_time)}")
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        if test_loader is not None:
            test_loss = evaluate_model(model, test_loader, criterion, device, use_amp)
            logger.info(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
            stats['test_losses'].append(test_loss)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % checkpoint_every == 0 or epoch == epochs - 1:
            save_checkpoint(
                epoch,
                model,
                optimizer,
                scheduler,
                scaler,
                stats,
                model_save_dir,
                name=f"model_epoch_{epoch+1}",
                is_best=(epoch == epochs - 1)
            )
        
        # æ›´æ–°epochå¼€å§‹æ—¶é—´
        epoch_start_time = time.time()
    
    # è®­ç»ƒç»“æŸï¼Œç»˜åˆ¶å›¾è¡¨å’Œæ€»ç»“
    print_section_header("è®­ç»ƒå®Œæˆ")
    
    # ç»˜åˆ¶è®­ç»ƒç»Ÿè®¡å›¾è¡¨
    log_info("ç”Ÿæˆè®­ç»ƒç»Ÿè®¡å›¾è¡¨...")
    plot_training_stats(stats, model_save_dir)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os_local.path.join(model_save_dir, f"final_model_v{VERSION}.pt")
    torch.save(model.state_dict(), final_model_path)
    log_success(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    
    # æ‰“å°è®­ç»ƒæ€»ç»“
    training_summary = [
        f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»è®­ç»ƒè½®æ•°: {epoch + 1}",
        f"æœ€ä½³æµ‹è¯•æŸå¤±: {best_loss:.4f}",
        f"æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}"
    ]
    
    log_info("è®­ç»ƒæ€»ç»“:")
    for summary in training_summary:
        logger.info(summary)
    
    return model

def evaluate_model(model, test_loader, criterion, device, use_amp=True):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        progress_bar = tqdm(test_loader, desc="è¯„ä¼°ä¸­", colour="blue", 
                           ncols=100, leave=False)
        
        for step, batch in enumerate(progress_bar):
            try:
                # å¤„ç†æ–°æ—§ä¸¤ç§æ•°æ®æ ¼å¼ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                if isinstance(batch, dict):
                    # æ—§æ ¼å¼ï¼šå­—å…¸å½¢å¼{'input_ids': tensor, 'target_ids': tensor}
                    input_ids = batch['input_ids'].to(device)
                    target_ids = batch['target_ids'].to(device)
                elif isinstance(batch, tuple) and len(batch) == 2:
                    # æ–°æ ¼å¼ï¼šå…ƒç»„å½¢å¼(input_tensor, target_tensor)
                    input_ids, target_ids = batch
                    input_ids = input_ids.to(device)
                    target_ids = target_ids.to(device)
                else:
                    logger.warning(f"æœªçŸ¥çš„æ‰¹å¤„ç†æ ¼å¼: {type(batch)}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # æ··åˆç²¾åº¦è¯„ä¼°
                with autocast(enabled=use_amp):
                    # å‰å‘è®¡ç®—
                    outputs = model(input_ids)
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))
                
                total_loss += loss.item()
                average_loss = total_loss / (step + 1)
                
                progress_bar.set_postfix({
                    'loss': f"{average_loss:.4f}"
                })
                
            except Exception as e:
                logger.error(f"è¯„ä¼°ä¸­å‡ºé”™ (step {step}): {str(e)}")
                continue
    
    return total_loss / len(test_loader)

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    logger = setup_logger()  # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡
    
    # ç®€åŒ–æ¬¢è¿ä¿¡æ¯
    logger.info(f"çµçŒ«å¢¨éŸµè¯­è¨€æ¨¡å‹è®­ç»ƒç³»ç»Ÿ v{VERSION}")
    logger.info("è‡ªåŠ¨é…ç½®æ¨¡å¼ï¼šå°†ä½¿ç”¨æœ€ä¼˜å‚æ•°å’Œå¯ç”¨è®¾å¤‡")
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="çµçŒ«è¯­è¨€æ¨¡å‹è®­ç»ƒ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--train_file", type=str, default="dataset/train_data_train.jsonl", help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--test_file", type=str, default=None, help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--tokenizer_path", type=str, default="tokenizer.json", help="åˆ†è¯å™¨è·¯å¾„")
    parser.add_argument("--context_length", type=int, default=512, help="ä¸Šä¸‹æ–‡é•¿åº¦")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--d_model", type=int, default=768, help="æ¨¡å‹ç»´åº¦")
    parser.add_argument("--nhead", type=int, default=12, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--num_layers", type=int, default=12, help="Transformerå±‚æ•°")
    parser.add_argument("--dim_feedforward", type=int, default=3072, help="å‰é¦ˆç½‘ç»œç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutç‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--checkpoint_every", type=int, default=1, help="æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹")
    parser.add_argument("--model_save_dir", type=str, default="model_weights", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--use_amp", dest='use_amp', action='store_true', help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--no_use_amp", dest='use_amp', action='store_false', help="ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.set_defaults(use_amp=True)
    parser.add_argument("--no_cuda", action="store_true", help="ç¦ç”¨CUDA")
    parser.add_argument("--accumulation_steps", type=int, default=4, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡ç³»æ•°")
    
    # æ¢å¤è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument("--resume_from", type=str, default=None, help="ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--auto_resume", action="store_true", default=True, help="è‡ªåŠ¨ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--no_auto_resume", dest='auto_resume', action='store_false', help="ç¦ç”¨è‡ªåŠ¨æ¢å¤è®­ç»ƒ")
    
    # æ€§èƒ½ä¸ç”µæºç®¡ç†å‚æ•°
    parser.add_argument("--night_mode", action="store_true", default=True, help="å¯ç”¨å¤œé—´ä½åŠŸè€—æ¨¡å¼")
    parser.add_argument("--no_night_mode", dest='night_mode', action='store_false', help="ç¦ç”¨å¤œé—´ä½åŠŸè€—æ¨¡å¼")
    parser.add_argument("--save_on_interrupt", action="store_true", default=True, help="ä¸­æ–­æ—¶ä¿å­˜æ£€æŸ¥ç‚¹")
    parser.add_argument("--no_save_on_interrupt", dest='save_on_interrupt', action='store_false', help="ç¦ç”¨ä¸­æ–­æ—¶ä¿å­˜æ£€æŸ¥ç‚¹")
    parser.add_argument("--clean-before-run", action="store_true", default=False, help="åœ¨å¼€å§‹è®­ç»ƒå‰æ¸…ç†æ—§çš„æ—¥å¿—å’Œå›¾è¡¨")
    
    args = parser.parse_args()
    
    # --- å¼€å§‹å‰çš„æ¸…ç†å·¥ä½œ --- #
    if args.clean_before_run:
        logger.info("æ‰§è¡Œå¼€å§‹å‰çš„æ¸…ç†ä»»åŠ¡ (--clean-before-run)...")
        # 1. æ¸…ç†è®­ç»ƒæ—¥å¿—ç›®å½•
        train_log_dir = Path("logs/train_model")
        if train_log_dir.exists():
            try:
                shutil.rmtree(train_log_dir)
                logger.info(f"å·²æ¸…ç†è®­ç»ƒæ—¥å¿—ç›®å½•: {train_log_dir}")
                # æ¸…ç†åé‡æ–°åˆ›å»ºæ—¥å¿—ç›®å½•ï¼Œé¿å… setup_logger å¤±è´¥
                train_log_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.warning(f"æ¸…ç†è®­ç»ƒæ—¥å¿—ç›®å½• {train_log_dir} æ—¶å‡ºé”™: {e}")
        
        # 2. æ¸…ç†æ—§çš„ç»Ÿè®¡å›¾è¡¨
        stats_dir = Path(args.model_save_dir)
        cleaned_plots = 0
        if stats_dir.exists():
            for plot_file in stats_dir.glob("training_stats_*.png"):
                try:
                    plot_file.unlink()
                    cleaned_plots += 1
                except Exception as e:
                    logger.warning(f"åˆ é™¤å›¾è¡¨ {plot_file} æ—¶å‡ºé”™: {e}")
            for plot_file in stats_dir.glob("training_stats_*.pdf"):
                try:
                    plot_file.unlink()
                    cleaned_plots += 1
                except Exception as e:
                    logger.warning(f"åˆ é™¤å›¾è¡¨ {plot_file} æ—¶å‡ºé”™: {e}")
            if cleaned_plots > 0:
                logger.info(f"å·²æ¸…ç† {cleaned_plots} ä¸ªæ—§çš„ç»Ÿè®¡å›¾è¡¨æ–‡ä»¶äº {stats_dir}")
        logger.info("å¼€å§‹å‰çš„æ¸…ç†ä»»åŠ¡å®Œæˆã€‚")

    # è®¾ç½®è®¾å¤‡
    if args.no_cuda:
        device = torch.device("cpu")
    else:
        if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device("mps")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")
    
    # è®­ç»ƒæ¨¡å‹
    train_model(
        train_file=args.train_file,
        test_file=args.test_file,
        model_save_dir=args.model_save_dir,
        tokenizer_path=args.tokenizer_path,
        context_length=args.context_length,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        epochs=args.epochs,
        d_model=args.d_model,
        nhead=args.nhead,
        num_layers=args.num_layers,
        dim_feedforward=args.dim_feedforward,
        dropout=args.dropout,
        use_amp=args.use_amp,
        checkpoint_every=args.checkpoint_every,
        accumulation_steps=args.accumulation_steps,
        max_grad_norm=args.max_grad_norm,
        weight_decay=args.weight_decay,
        device=device,
        resume_from=args.resume_from,
        auto_resume=args.auto_resume,
        night_mode=args.night_mode,
        save_on_interrupt=args.save_on_interrupt
    )